{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ad451/Stackoverflow_tag_generator/blob/main/StackOverflow_Tag_Generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLtFomOOTrCE"
      },
      "source": [
        "\n",
        "**Importing the required modules**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "fy7GyiItTYJx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "579af45e-4211-4741-a92d-e0e8fc3ab0e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import requests\n",
        "import random\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "import warnings; warnings.simplefilter('ignore')\n",
        "import re\n",
        "import string\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import ToktokTokenizer\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk import pos_tag\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils.class_weight import compute_sample_weight\n",
        "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "import gensim\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "\n",
        "nltk.download('averaged_perceptron_tagger') # required for parts of speech\n",
        "nltk.download('wordnet') # required for parts of speech\n",
        "nltk.download('stopwords') #download the stopwords\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAaTn6ZCW3CB"
      },
      "source": [
        "**Web Scraping the current set of questions for testing the model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NKcc1fyLhTBt",
        "outputId": "25598b37-cea6-4af0-80f7-d6990fafa540"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting patool\n",
            "  Downloading patool-1.12-py2.py3-none-any.whl (77 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/77.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.5/77.5 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: patool\n",
            "Successfully installed patool-1.12\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "Questions=[] #array to store the questions\n",
        "\n",
        "for pageNumber in range(1,401):\n",
        "    response=requests.get(\"https://stackoverflow.com/questions\",params={\"tab\":\"newest\",\"page\":pageNumber,\"pagesize\":50})\n",
        "\n",
        "    data=BeautifulSoup(response.text,'html.parser' ) #parsing the html text\n",
        "\n",
        "    req_data=data.find(id=\"questions\")\n",
        "\n",
        "    new_data=req_data.find_all(\"h3\", class_=\"s-post-summary--content-title\") # the tag that contains the question info\n",
        "\n",
        "\n",
        "\n",
        "    for element in new_data:\n",
        "        link=element.a.attrs['href']\n",
        "\n",
        "        response=requests.get(f\"https://stackoverflow.com/{link}\")  #fetching the content related to the question\n",
        "\n",
        "        data_questionwise=BeautifulSoup(response.text,'html.parser')\n",
        "\n",
        "        question_wise_title=data_questionwise.find(\"div\",id=\"question-header\").h1.a.string #title\n",
        "\n",
        "        question_wise_desc=data_questionwise.find(\"div\",class_=\"s-prose js-post-body\") #description\n",
        "\n",
        "        all_paragraphs=question_wise_desc.find_all(\"p\")\n",
        "\n",
        "        total_description_question_wise=\"\"\n",
        "\n",
        "        for para in all_paragraphs:\n",
        "            total_description_question_wise+=para.text\n",
        "\n",
        "        Final_content=question_wise_title+\"\"+total_description_question_wise  #concatenating the title and description\n",
        "\n",
        "        tag_question_wise=data_questionwise.find(\"ul\",class_=\"ml0 list-ls-none js-post-tag-list-wrapper d-inline\").li.text #tag\n",
        "\n",
        "        Questions.append([Final_content,tag_question_wise])\n",
        "\n",
        "    print(pageNumber)   #checking which page questions have been fetched yet\n",
        "\n",
        "print(len(Questions))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8MnaCTvXb8q"
      },
      "source": [
        "**Sql code to combine the input questions and tags table**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oXOsnSFrjn9b"
      },
      "outputs": [],
      "source": [
        "###################################### Code for inner combine ######################################\n",
        "df1 = pd.read_csv('Questions.csv', encoding='ISO-8859-1')\n",
        "df2 = pd.read_csv('Tags.csv', encoding='ISO-8859-1')\n",
        "\n",
        "# combined dataframe of questiontags\n",
        "df3 = df1.set_index('Id').join(df2.set_index('Id'))\n",
        "df3 = df3.reset_index()\n",
        "df4 = pd.DataFrame(df3.Id.value_counts())\n",
        "df4 = df4.reset_index()\n",
        "\n",
        "# df5 is the final combined dataframe of questiontags. Randomly picked datapoints from df3\n",
        "store = {}\n",
        "df5 = pd.DataFrame()\n",
        "n_train = 20000\n",
        "for i in range(n_train):\n",
        "    key = 0\n",
        "    while(key==0):\n",
        "        a = random.randint(df4.shape[0])\n",
        "        if a in store:\n",
        "            continue\n",
        "        else:\n",
        "            key==1\n",
        "            store[a] = 1\n",
        "            ind = df3[df3.Id==df4.iloc[a,0]].first_valid_index()\n",
        "            df5 =  pd.concat([df5,pd.DataFrame(df3.iloc[ind]).T],ignore_index=True)\n",
        "            break\n",
        "\n",
        "###################################### Code for removing angular brackets from the \"Body\" column ######################################\n",
        "\n",
        "for j in range(df5.shape[0]):\n",
        "    ans=\" \"\n",
        "    curr = 0\n",
        "    flag = 0\n",
        "    while(curr<len(df5['Body'][j])):\n",
        "        if( df5['Body'][j][curr]=='<'):\n",
        "            flag = 1\n",
        "            curr = curr + 1\n",
        "            continue\n",
        "        if( df5['Body'][j][curr]=='>'):\n",
        "            flag = 0\n",
        "            curr = curr + 1\n",
        "            continue\n",
        "        if(flag == 1):\n",
        "            curr = curr + 1\n",
        "        if(flag == 0):\n",
        "            ans = ans + df5['Body'][j][curr]\n",
        "            curr = curr + 1\n",
        "    df1['Title'][j] += ans\n",
        "\n",
        "df6 = pd.concat([df5.iloc[:,6], df5.iloc[:,8]], axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kp5z4pMGXlBw"
      },
      "source": [
        "**Machine learning part (Preprocessing and training)**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "VKY3pgGoXsJg"
      },
      "outputs": [],
      "source": [
        "train_data=pd.read_csv(\"questiontags_train.csv\")\n",
        "test_data=pd.read_csv(\"questiontags_test.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "CwaW-h2d4PaE"
      },
      "outputs": [],
      "source": [
        "#rename the columns of the train dataset\n",
        "\n",
        "train_data.drop(train_data.columns[0],axis=1,inplace=True)\n",
        "\n",
        "\n",
        "train_data.rename(columns={\"Title\":\"Questions\",\"Tag\":\"Tags\"},inplace=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "xZcsg-0j57mm"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "#code to include around 10000 more questions to the train dataset from the test dataset\n",
        "v={}\n",
        "count=0\n",
        "while count!=10000:\n",
        "    idx=random.randint(0,test_data.shape[0]-1)\n",
        "    if idx not in v:\n",
        "      train_data=train_data.append(test_data.loc[idx],ignore_index=True)\n",
        "      test_data.drop(idx,inplace=True,axis=0)\n",
        "      count+=1\n",
        "      v[idx]=1\n",
        "    else:\n",
        "      continue\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "0_RPmQDS7raL"
      },
      "outputs": [],
      "source": [
        "train_data['Questions']=train_data['Questions'].str.replace('</p>',' ')\n",
        "train_data['Questions']=train_data['Questions'].str.replace('\\n',' ')\n",
        "train_data['Questions']=train_data['Questions'].str.replace('</a>',' ')\n",
        "train_data['Questions']=train_data['Questions'].apply(lambda x : re.sub('(https?:\\/\\/)?([\\da-z\\.-]+)\\.([a-z\\.]{2,6})([\\/\\w \\.-]*)' , ' ' , x)) #removing any urls\n",
        "train_data['Questions'] = train_data['Questions'].apply(lambda x: ' '.join([w for w in x.split() if len(w) > 3])) #removes small length words (len<3)\n",
        "train_data['Questions']=train_data['Questions'].str.replace('<a href=\" \">','')\n",
        "\n",
        "train_data['Questions']=train_data['Questions'].apply(lambda x : x.lower()) #coverting to lowercase\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "kGvOPabMHCHF"
      },
      "outputs": [],
      "source": [
        "test_data['Questions']=test_data['Questions'].str.replace('</p>',' ')\n",
        "test_data['Questions']=test_data['Questions'].str.replace('\\n',' ')\n",
        "test_data['Questions']=test_data['Questions'].str.replace('</a>',' ')\n",
        "test_data['Questions'] = test_data['Questions'].apply(lambda x: ' '.join([w for w in x.split() if len(w) > 3]))\n",
        "test_data['Questions']=test_data['Questions'].str.replace('<a href=\" \">','')\n",
        "test_data['Questions']=test_data['Questions'].apply(lambda x : x.lower())\n",
        "test_data['Questions']=test_data['Questions'].apply(lambda x : re.sub('(https?:\\/\\/)?([\\da-z\\.-]+)\\.([a-z\\.]{2,6})([\\/\\w \\.-]*)' , ' ' , x)) #removing any urls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G5fomFACHLes",
        "outputId": "92aed7d0-97af-4b4b-8886-cdfc4ab3e339"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "#removing the stop words and the punctuations from test and train dataset\n",
        "\n",
        "\n",
        "stop_words=set(nltk.corpus.stopwords.words('english'))\n",
        "train_data['Questions']=train_data['Questions'].apply(lambda x:' '.join([w for w in x.split() if w not in stop_words]))\n",
        "test_data['Questions']=test_data['Questions'].apply(lambda x:' '.join([w for w in x.split() if w not in stop_words]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "QpzdXyd2OPVt"
      },
      "outputs": [],
      "source": [
        "#removing the punctuations from test and train dataset\n",
        "\n",
        "\n",
        "punctuations = string.punctuation\n",
        "train_data['Questions']=train_data['Questions'].apply(lambda x:' '.join([''.join([char for char in w if char not in punctuations]) for w in x.split()]))\n",
        "test_data['Questions']=test_data['Questions'].apply(lambda x:' '.join([''.join([char for char in w if char not in punctuations]) for w in x.split()]))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "xdBZ0dpiX9n4"
      },
      "outputs": [],
      "source": [
        "test_data=test_data.reset_index()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "m232NAu4-HE7"
      },
      "outputs": [],
      "source": [
        "\n",
        "test_data.drop(['index'],inplace=True,axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vw5EEzCpYLQl",
        "outputId": "c57b19f9-1095-4943-acf4-ce471d215206"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "yes\n",
            "yes\n",
            "yes\n",
            "yes\n",
            "yes\n",
            "yes\n"
          ]
        }
      ],
      "source": [
        "#dropping the rows with empty values of Question after filtering\n",
        "for j in range(len(test_data['Questions'])):\n",
        "  if len(test_data['Questions'][j])==0:\n",
        "     test_data.drop(j,inplace=True)\n",
        "\n",
        "for j in range(len(train_data['Questions'])):\n",
        "  if len(train_data['Questions'][j])==0:\n",
        "     train_data.drop(j,inplace=True)\n",
        "     print(\"yes\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "hz8UAEsjaJt2"
      },
      "outputs": [],
      "source": [
        "test_data=test_data.reset_index()\n",
        "train_data=train_data.reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "zMtySQj_bfXt"
      },
      "outputs": [],
      "source": [
        "test_data.drop(['index'],inplace=True,axis=1)\n",
        "train_data.drop(['index'],inplace=True,axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "Z3vqRKOKbk_N"
      },
      "outputs": [],
      "source": [
        "#using lemmatization on the questions of the train and test dataset\n",
        "def lemmatization(text):\n",
        "    pos_dict = {\n",
        "        'N': 'n',  # Noun\n",
        "        'V': 'v',  # Verb\n",
        "        'R': 'r',  # Adverb\n",
        "        'J': 'a'   # Adjective\n",
        "    }\n",
        "    pos_tags = pos_tag(text)\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemma=[]\n",
        "    for word, tag in pos_tags:\n",
        "        if (tag[0].upper() not in pos_dict.keys()):\n",
        "          pos='n'\n",
        "        else:\n",
        "          pos= pos_dict[tag[0].upper()]\n",
        "        lemma.append(lemmatizer.lemmatize(word,pos=pos))\n",
        "    return lemma\n",
        "\n",
        "train_data['Questions']=train_data['Questions'].apply(lambda x : lemmatization(x.split()))\n",
        "test_data['Questions']=test_data['Questions'].apply(lambda x : lemmatization(x.split()))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Exploration with dev of Base Model**"
      ],
      "metadata": {
        "id": "MKRKercrIKUn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "questions = train_data['Questions'].tolist()\n",
        "tags = train_data['Tags'].tolist()\n",
        "\n",
        "print('The total number of words in the data is: ', sum([len(text.split()) for text in questions]))\n",
        "\n",
        "def tokenize_question(text):\n",
        "    return text.split()\n",
        "\n",
        "question_vect = CountVectorizer(tokenizer=tokenize_question)\n",
        "questions=question_vect.fit_transform(questions)\n",
        "\n",
        "print('The number of words in the vocabulary is: ', len(question_vect.vocabulary_))\n",
        "\n",
        "def tokenize_tags(text):\n",
        "    return text.split('|')\n",
        "\n",
        "max_tags = 100\n",
        "tags_vect = CountVectorizer(tokenizer=tokenize_tags, max_features=max_tags)\n",
        "tags = tags_vect.fit_transform(tags)\n",
        "tags = tags.toarray()\n",
        "print('Number of tags: ', len(tags_vect.vocabulary_))\n",
        "\n",
        "tags_token = tags_vect.get_feature_names_out()\n",
        "tag_frequency = tags.sum(axis=0)\n",
        "print('The list of tags with frequency is: ')\n",
        "print(dict(zip(tags_token, tag_frequency)))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pL0HGU975TWq",
        "outputId": "3c9df8a6-961e-491e-cfab-bb6cf432edff"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The total number of words in the data is:  2192542\n",
            "The number of words in the vocabulary is:  272590\n",
            "Number of tags:  100\n",
            "The list of tags with frequency is: \n",
            "{'.htaccess': 30, '.net': 153, 'actionscript-3': 39, 'ajax': 42, 'algorithm': 66, 'amazon-web-services': 128, 'android': 1403, 'angular': 115, 'angularjs': 164, 'apache': 53, 'apache-spark': 32, 'arrays': 51, 'asp.net': 223, 'asp.net-mvc': 100, 'assembly': 28, 'azure': 127, 'bash': 88, 'batch-file': 27, 'c': 403, 'c#': 2013, 'c++': 968, 'cakephp': 28, 'css': 259, 'database': 85, 'delphi': 55, 'django': 150, 'docker': 112, 'eclipse': 64, 'elasticsearch': 40, 'excel': 212, 'facebook': 54, 'firebase': 35, 'flutter': 202, 'git': 147, 'go': 67, 'google-apps-script': 42, 'hadoop': 43, 'haskell': 35, 'html': 480, 'html5': 39, 'image': 26, 'ios': 681, 'iphone': 279, 'java': 2339, 'javascript': 2850, 'jquery': 532, 'jsf': 27, 'json': 86, 'kubernetes': 48, 'laravel': 58, 'linux': 195, 'lua': 28, 'machine-learning': 29, 'magento': 28, 'matlab': 90, 'mongodb': 71, 'ms-access': 27, 'mysql': 357, 'next.js': 48, 'node.js': 336, 'objective-c': 171, 'oracle': 50, 'osx': 44, 'performance': 27, 'perl': 61, 'php': 1714, 'postgresql': 91, 'powershell': 73, 'python': 2254, 'python-3.x': 113, 'qt': 30, 'r': 497, 'react-native': 58, 'reactjs': 320, 'regex': 103, 'rest': 27, 'ruby': 143, 'ruby-on-rails': 376, 'rust': 64, 'scala': 81, 'shell': 30, 'spring': 75, 'spring-boot': 44, 'sql': 477, 'sql-server': 141, 'svn': 26, 'swift': 122, 'typescript': 107, 'ubuntu': 32, 'unity-game-engine': 29, 'vb.net': 94, 'vba': 47, 'visual-studio': 47, 'visual-studio-code': 30, 'wcf': 34, 'windows': 148, 'wordpress': 92, 'wpf': 82, 'xcode': 52, 'xml': 95}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tag_count = [[x[0],x[1]] for x in zip(tags_token, tag_frequency)]\n",
        "tag_count = sorted(tag_count, key = lambda x: x[1], reverse=True)\n",
        "\n",
        "print('Rank     Tag      Count')\n",
        "for i in range(100):\n",
        "    print(\"%d   %s   %d\" % (i+1, tag_count[i][0], tag_count[i][1]))"
      ],
      "metadata": {
        "id": "HdkSV-b_6HSh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e7ce1eb-95d8-4e80-8dfb-903c7de1ca2e"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rank     Tag      Count\n",
            "1   javascript   2850\n",
            "2   java   2339\n",
            "3   python   2254\n",
            "4   c#   2013\n",
            "5   php   1714\n",
            "6   android   1403\n",
            "7   c++   968\n",
            "8   ios   681\n",
            "9   jquery   532\n",
            "10   r   497\n",
            "11   html   480\n",
            "12   sql   477\n",
            "13   c   403\n",
            "14   ruby-on-rails   376\n",
            "15   mysql   357\n",
            "16   node.js   336\n",
            "17   reactjs   320\n",
            "18   iphone   279\n",
            "19   css   259\n",
            "20   asp.net   223\n",
            "21   excel   212\n",
            "22   flutter   202\n",
            "23   linux   195\n",
            "24   objective-c   171\n",
            "25   angularjs   164\n",
            "26   .net   153\n",
            "27   django   150\n",
            "28   windows   148\n",
            "29   git   147\n",
            "30   ruby   143\n",
            "31   sql-server   141\n",
            "32   amazon-web-services   128\n",
            "33   azure   127\n",
            "34   swift   122\n",
            "35   angular   115\n",
            "36   python-3.x   113\n",
            "37   docker   112\n",
            "38   typescript   107\n",
            "39   regex   103\n",
            "40   asp.net-mvc   100\n",
            "41   xml   95\n",
            "42   vb.net   94\n",
            "43   wordpress   92\n",
            "44   postgresql   91\n",
            "45   matlab   90\n",
            "46   bash   88\n",
            "47   json   86\n",
            "48   database   85\n",
            "49   wpf   82\n",
            "50   scala   81\n",
            "51   spring   75\n",
            "52   powershell   73\n",
            "53   mongodb   71\n",
            "54   go   67\n",
            "55   algorithm   66\n",
            "56   eclipse   64\n",
            "57   rust   64\n",
            "58   perl   61\n",
            "59   laravel   58\n",
            "60   react-native   58\n",
            "61   delphi   55\n",
            "62   facebook   54\n",
            "63   apache   53\n",
            "64   xcode   52\n",
            "65   arrays   51\n",
            "66   oracle   50\n",
            "67   kubernetes   48\n",
            "68   next.js   48\n",
            "69   vba   47\n",
            "70   visual-studio   47\n",
            "71   osx   44\n",
            "72   spring-boot   44\n",
            "73   hadoop   43\n",
            "74   ajax   42\n",
            "75   google-apps-script   42\n",
            "76   elasticsearch   40\n",
            "77   actionscript-3   39\n",
            "78   html5   39\n",
            "79   firebase   35\n",
            "80   haskell   35\n",
            "81   wcf   34\n",
            "82   apache-spark   32\n",
            "83   ubuntu   32\n",
            "84   .htaccess   30\n",
            "85   qt   30\n",
            "86   shell   30\n",
            "87   visual-studio-code   30\n",
            "88   machine-learning   29\n",
            "89   unity-game-engine   29\n",
            "90   assembly   28\n",
            "91   cakephp   28\n",
            "92   lua   28\n",
            "93   magento   28\n",
            "94   batch-file   27\n",
            "95   jsf   27\n",
            "96   ms-access   27\n",
            "97   performance   27\n",
            "98   rest   27\n",
            "99   image   26\n",
            "100   svn   26\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# X_test = test_data['Questions'].tolist()\n",
        "# y_test = test_data['Tags'].tolist()"
      ],
      "metadata": {
        "id": "0EGEl52m9p-J"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# X_train, X_test, y_train, y_test = train_test_split(questions, tags,\n",
        "#                                                     test_size=0.2,\n",
        "#                                                     random_state=747)\n",
        "\n",
        "# X_val, X_test, y_val, y_test = train_test_split(X_test, y_test,\n",
        "#                                                     test_size=0.5,\n",
        "#                                                     random_state=747)"
      ],
      "metadata": {
        "id": "WADg9CVe8CX6"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_metrics(y_test, y_predicted, print_metrics=True):\n",
        "\n",
        "    accuracy = accuracy_score(y_test, y_predicted)\n",
        "    precision = precision_score(y_test, y_predicted, average='weighted')\n",
        "    recall = recall_score(y_test, y_predicted, average='weighted')\n",
        "    f1 = f1_score(y_test, y_predicted, average='weighted')\n",
        "\n",
        "    if print_metrics:\n",
        "        print(\"f1: %.3f - precision: %.3f - recall: %.3f - accuracy: %.3f\" % (\n",
        "            f1, precision, recall, accuracy))\n",
        "    return f1, precision, recall, accuracy"
      ],
      "metadata": {
        "id": "IeI6PBJw97t4"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "XKqw3_Zcs7C1",
        "outputId": "19f0b29e-22c1-4f0c-886a-f88117cf7850"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-96c94e763edb>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/sparse/_base.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    769\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetnnz\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 771\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattr\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" not found\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    772\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: tolist not found"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# def tokenize_question(text):\n",
        "#     return text.split()\n",
        "\n",
        "tfidf_vect = TfidfVectorizer(tokenizer=tokenize_question,\n",
        "                               stop_words='english',\n",
        "                               min_df=2,\n",
        "                               max_df=0.5)\n",
        "\n",
        "X_train_tfidf = tfidf_vect.fit_transform(train_data[\"Questions\"])\n",
        "X_test_tfidf = tfidf_vect.transform(test_data[\"Questions\"])\n",
        "print('The number of words in the vocabulary is: ', len(tfidf_vect.vocabulary_))\n",
        "\n",
        "feature_names = tfidf_vect.get_feature_names_out()\n",
        "\n",
        "# for i, feature in enumerate(feature_names):\n",
        "#     print(\"Feature:\", feature)\n",
        "#     print(\"Train TF-IDF:\", max(X_train_tfidf[:, i]))\n",
        "#     if i==20:\n",
        "#       break\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KKLqkfsa-bVI",
        "outputId": "6f58d7ba-2988-43cd-cf84-16cff8763dbe"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The number of words in the vocabulary is:  47064\n",
            "47064\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "tfidf_log_clf = OneVsRestClassifier(LogisticRegression())\n",
        "\n",
        "tfidf_log_clf.fit(X_train_tfidf, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "id": "Vj1D6BPbG0Hv",
        "outputId": "ddd2fdd1-1679-49d3-ae71-dcba17d10385"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 27.6 s, sys: 28.5 s, total: 56.1 s\n",
            "Wall time: 32 s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OneVsRestClassifier(estimator=LogisticRegression())"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>OneVsRestClassifier(estimator=LogisticRegression())</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">OneVsRestClassifier</label><div class=\"sk-toggleable__content\"><pre>OneVsRestClassifier(estimator=LogisticRegression())</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_tfidf_predict = tfidf_log_clf.predict(X_train_tfidf)\n",
        "eval_metrics(y_train, y_train_tfidf_predict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eoa36tt0HFSH",
        "outputId": "0af1e221-7738-4296-f7c2-4c974d524a21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "f1: 0.287 - precision: 0.705 - recall: 0.192 - accuracy: 0.325\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.2869979496204687,\n",
              " 0.7048346764490622,\n",
              " 0.19239736441966548,\n",
              " 0.3247759949989581)"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_test_tfidf_predict = tfidf_log_clf.predict(X_test_tfidf)\n",
        "eval_metrics(y_test, y_test_tfidf_predict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VaPKwJwDHP8P",
        "outputId": "8f014bc0-27e6-4cf6-ef11-110db577d36f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "f1: 0.246 - precision: 0.563 - recall: 0.164 - accuracy: 0.287\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.24568413711051668, 0.5629343082208758, 0.1643342662934826, 0.287)"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}