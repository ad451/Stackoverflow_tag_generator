{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ad451/Stackoverflow_tag_generator/blob/main/StackOverflow_Tag_Generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLtFomOOTrCE"
      },
      "source": [
        "\n",
        "**Importing the required modules**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "fy7GyiItTYJx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b1568fd-1419-4de9-c061-057adc3b7d31"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import requests\n",
        "import random\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "import warnings; warnings.simplefilter('ignore')\n",
        "import re\n",
        "import string\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import ToktokTokenizer\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk import pos_tag\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils.class_weight import compute_sample_weight\n",
        "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn import model_selection, naive_bayes, svm\n",
        "import gensim\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "\n",
        "nltk.download('averaged_perceptron_tagger') # required for parts of speech\n",
        "nltk.download('wordnet') # required for parts of speech\n",
        "nltk.download('stopwords') #download the stopwords\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAaTn6ZCW3CB"
      },
      "source": [
        "**Web Scraping the current set of questions for testing the model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NKcc1fyLhTBt",
        "outputId": "25598b37-cea6-4af0-80f7-d6990fafa540"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting patool\n",
            "  Downloading patool-1.12-py2.py3-none-any.whl (77 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/77.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.5/77.5 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: patool\n",
            "Successfully installed patool-1.12\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "Questions=[] #array to store the questions\n",
        "\n",
        "for pageNumber in range(1,401):\n",
        "    response=requests.get(\"https://stackoverflow.com/questions\",params={\"tab\":\"newest\",\"page\":pageNumber,\"pagesize\":50})\n",
        "\n",
        "    data=BeautifulSoup(response.text,'html.parser' ) #parsing the html text\n",
        "\n",
        "    req_data=data.find(id=\"questions\")\n",
        "\n",
        "    new_data=req_data.find_all(\"h3\", class_=\"s-post-summary--content-title\") # the tag that contains the question info\n",
        "\n",
        "\n",
        "\n",
        "    for element in new_data:\n",
        "        link=element.a.attrs['href']\n",
        "\n",
        "        response=requests.get(f\"https://stackoverflow.com/{link}\")  #fetching the content related to the question\n",
        "\n",
        "        data_questionwise=BeautifulSoup(response.text,'html.parser')\n",
        "\n",
        "        question_wise_title=data_questionwise.find(\"div\",id=\"question-header\").h1.a.string #title\n",
        "\n",
        "        question_wise_desc=data_questionwise.find(\"div\",class_=\"s-prose js-post-body\") #description\n",
        "\n",
        "        all_paragraphs=question_wise_desc.find_all(\"p\")\n",
        "\n",
        "        total_description_question_wise=\"\"\n",
        "\n",
        "        for para in all_paragraphs:\n",
        "            total_description_question_wise+=para.text\n",
        "\n",
        "        Final_content=question_wise_title+\"\"+total_description_question_wise  #concatenating the title and description\n",
        "\n",
        "        tag_question_wise=data_questionwise.find(\"ul\",class_=\"ml0 list-ls-none js-post-tag-list-wrapper d-inline\").li.text #tag\n",
        "\n",
        "        Questions.append([Final_content,tag_question_wise])\n",
        "\n",
        "    print(pageNumber)   #checking which page questions have been fetched yet\n",
        "\n",
        "print(len(Questions))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8MnaCTvXb8q"
      },
      "source": [
        "**Sql code to combine the input questions and tags table**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oXOsnSFrjn9b"
      },
      "outputs": [],
      "source": [
        "###################################### Code for inner combine ######################################\n",
        "df1 = pd.read_csv('Questions.csv', encoding='ISO-8859-1')\n",
        "df2 = pd.read_csv('Tags.csv', encoding='ISO-8859-1')\n",
        "\n",
        "# combined dataframe of questiontags\n",
        "df3 = df1.set_index('Id').join(df2.set_index('Id'))\n",
        "df3 = df3.reset_index()\n",
        "df4 = pd.DataFrame(df3.Id.value_counts())\n",
        "df4 = df4.reset_index()\n",
        "\n",
        "# df5 is the final combined dataframe of questiontags. Randomly picked datapoints from df3\n",
        "store = {}\n",
        "df5 = pd.DataFrame()\n",
        "n_train = 20000\n",
        "for i in range(n_train):\n",
        "    key = 0\n",
        "    while(key==0):\n",
        "        a = random.randint(df4.shape[0])\n",
        "        if a in store:\n",
        "            continue\n",
        "        else:\n",
        "            key==1\n",
        "            store[a] = 1\n",
        "            ind = df3[df3.Id==df4.iloc[a,0]].first_valid_index()\n",
        "            df5 =  pd.concat([df5,pd.DataFrame(df3.iloc[ind]).T],ignore_index=True)\n",
        "            break\n",
        "\n",
        "###################################### Code for removing angular brackets from the \"Body\" column ######################################\n",
        "\n",
        "for j in range(df5.shape[0]):\n",
        "    ans=\" \"\n",
        "    curr = 0\n",
        "    flag = 0\n",
        "    while(curr<len(df5['Body'][j])):\n",
        "        if( df5['Body'][j][curr]=='<'):\n",
        "            flag = 1\n",
        "            curr = curr + 1\n",
        "            continue\n",
        "        if( df5['Body'][j][curr]=='>'):\n",
        "            flag = 0\n",
        "            curr = curr + 1\n",
        "            continue\n",
        "        if(flag == 1):\n",
        "            curr = curr + 1\n",
        "        if(flag == 0):\n",
        "            ans = ans + df5['Body'][j][curr]\n",
        "            curr = curr + 1\n",
        "    df1['Title'][j] += ans\n",
        "\n",
        "df6 = pd.concat([df5.iloc[:,6], df5.iloc[:,8]], axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kp5z4pMGXlBw"
      },
      "source": [
        "**Machine learning part (Preprocessing and training)**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "VKY3pgGoXsJg"
      },
      "outputs": [],
      "source": [
        "train_data=pd.read_csv(\"questiontags_train.csv\")\n",
        "test_data=pd.read_csv(\"questiontags_test.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "CwaW-h2d4PaE"
      },
      "outputs": [],
      "source": [
        "#rename the columns of the train dataset\n",
        "\n",
        "train_data.drop(train_data.columns[0],axis=1,inplace=True)\n",
        "\n",
        "\n",
        "train_data.rename(columns={\"Title\":\"Questions\",\"Tag\":\"Tags\"},inplace=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "xZcsg-0j57mm"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "#code to include around 10000 more questions to the train dataset from the test dataset\n",
        "v={}\n",
        "count=0\n",
        "while count!=10000:\n",
        "    idx=random.randint(0,test_data.shape[0]-1)\n",
        "    if idx not in v:\n",
        "      train_data=train_data.append(test_data.loc[idx],ignore_index=True)\n",
        "      test_data.drop(idx,inplace=True,axis=0)\n",
        "      count+=1\n",
        "      v[idx]=1\n",
        "    else:\n",
        "      continue\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "0_RPmQDS7raL"
      },
      "outputs": [],
      "source": [
        "train_data['Questions']=train_data['Questions'].str.replace('</p>',' ')\n",
        "train_data['Questions']=train_data['Questions'].str.replace('\\n',' ')\n",
        "train_data['Questions']=train_data['Questions'].str.replace('</a>',' ')\n",
        "train_data['Questions']=train_data['Questions'].apply(lambda x : re.sub('(https?:\\/\\/)?([\\da-z\\.-]+)\\.([a-z\\.]{2,6})([\\/\\w \\.-]*)' , ' ' , x)) #removing any urls\n",
        "train_data['Questions'] = train_data['Questions'].apply(lambda x: ' '.join([w for w in x.split() if len(w) > 3])) #removes small length words (len<3)\n",
        "train_data['Questions']=train_data['Questions'].str.replace('<a href=\" \">','')\n",
        "\n",
        "train_data['Questions']=train_data['Questions'].apply(lambda x : x.lower()) #coverting to lowercase\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "kGvOPabMHCHF"
      },
      "outputs": [],
      "source": [
        "test_data['Questions']=test_data['Questions'].str.replace('</p>',' ')\n",
        "test_data['Questions']=test_data['Questions'].str.replace('\\n',' ')\n",
        "test_data['Questions']=test_data['Questions'].str.replace('</a>',' ')\n",
        "test_data['Questions'] = test_data['Questions'].apply(lambda x: ' '.join([w for w in x.split() if len(w) > 3]))\n",
        "test_data['Questions']=test_data['Questions'].str.replace('<a href=\" \">','')\n",
        "test_data['Questions']=test_data['Questions'].apply(lambda x : x.lower())\n",
        "test_data['Questions']=test_data['Questions'].apply(lambda x : re.sub('(https?:\\/\\/)?([\\da-z\\.-]+)\\.([a-z\\.]{2,6})([\\/\\w \\.-]*)' , ' ' , x)) #removing any urls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "G5fomFACHLes"
      },
      "outputs": [],
      "source": [
        "#removing the stop words and the punctuations from test and train dataset\n",
        "\n",
        "\n",
        "stop_words=set(nltk.corpus.stopwords.words('english'))\n",
        "train_data['Questions']=train_data['Questions'].apply(lambda x:' '.join([w for w in x.split() if w not in stop_words]))\n",
        "test_data['Questions']=test_data['Questions'].apply(lambda x:' '.join([w for w in x.split() if w not in stop_words]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "QpzdXyd2OPVt"
      },
      "outputs": [],
      "source": [
        "#removing the punctuations from test and train dataset\n",
        "\n",
        "\n",
        "punctuations = string.punctuation\n",
        "train_data['Questions']=train_data['Questions'].apply(lambda x:' '.join([''.join([char for char in w if char not in punctuations]) for w in x.split()]))\n",
        "test_data['Questions']=test_data['Questions'].apply(lambda x:' '.join([''.join([char for char in w if char not in punctuations]) for w in x.split()]))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "xdBZ0dpiX9n4"
      },
      "outputs": [],
      "source": [
        "test_data=test_data.reset_index()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "m232NAu4-HE7"
      },
      "outputs": [],
      "source": [
        "\n",
        "test_data.drop(['index'],inplace=True,axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vw5EEzCpYLQl",
        "outputId": "60758394-0f82-4547-e19f-2f45187bf016"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "yes\n",
            "yes\n",
            "yes\n",
            "yes\n",
            "yes\n"
          ]
        }
      ],
      "source": [
        "#dropping the rows with empty values of Question after filtering\n",
        "for j in range(len(test_data['Questions'])):\n",
        "  if len(test_data['Questions'][j])==0:\n",
        "     test_data.drop(j,inplace=True)\n",
        "\n",
        "for j in range(len(train_data['Questions'])):\n",
        "  if len(train_data['Questions'][j])==0:\n",
        "     train_data.drop(j,inplace=True)\n",
        "     print(\"yes\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "hz8UAEsjaJt2"
      },
      "outputs": [],
      "source": [
        "test_data=test_data.reset_index()\n",
        "train_data=train_data.reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "zMtySQj_bfXt"
      },
      "outputs": [],
      "source": [
        "test_data.drop(['index'],inplace=True,axis=1)\n",
        "train_data.drop(['index'],inplace=True,axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Z3vqRKOKbk_N"
      },
      "outputs": [],
      "source": [
        "#using lemmatization on the questions of the train and test dataset\n",
        "def lemmatization(text):\n",
        "    pos_dict = {\n",
        "        'N': 'n',  # Noun\n",
        "        'V': 'v',  # Verb\n",
        "        'R': 'r',  # Adverb\n",
        "        'J': 'a'   # Adjective\n",
        "    }\n",
        "    pos_tags = pos_tag(text)\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemma=[]\n",
        "    for word, tag in pos_tags:\n",
        "        if (tag[0].upper() not in pos_dict.keys()):\n",
        "          pos='n'\n",
        "        else:\n",
        "          pos= pos_dict[tag[0].upper()]\n",
        "        lemma.append(lemmatizer.lemmatize(word,pos=pos))\n",
        "    return lemma\n",
        "\n",
        "train_data['Questions']=train_data['Questions'].apply(lambda x : lemmatization(x.split()))\n",
        "test_data['Questions']=test_data['Questions'].apply(lambda x : lemmatization(x.split()))\n",
        "train_data[\"Questions\"]=train_data[\"Questions\"].apply(lambda x : \" \".join(x))\n",
        "\n",
        "test_data[\"Questions\"]=test_data[\"Questions\"].apply(lambda x : \" \".join(x))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Exploration with dev of Base Model**"
      ],
      "metadata": {
        "id": "MKRKercrIKUn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Analysing certain parameters about the train data\n",
        "\n",
        "def tokenize_question(text):\n",
        "    return text.split()\n",
        "\n",
        "def tokenize_tags(text):\n",
        "    return text.split('|')\n",
        "\n",
        "questions = train_data['Questions'].tolist()\n",
        "tags = train_data['Tags'].tolist()\n",
        "\n",
        "print('The total number of words in the data is: ', sum([len(text.split()) for text in questions]))\n",
        "\n",
        "\n",
        "\n",
        "question_vect = CountVectorizer(tokenizer=tokenize_question)\n",
        "questions=question_vect.fit_transform(questions)\n",
        "\n",
        "print('The number of words in the vocabulary is: ', len(question_vect.vocabulary_))\n",
        "\n",
        "\n",
        "# Finding the most common tags\n",
        "\n",
        "max_tags = 100\n",
        "tags_vect = CountVectorizer(tokenizer=tokenize_tags, max_features=max_tags)\n",
        "tags = tags_vect.fit_transform(tags)\n",
        "tags = tags.toarray()\n",
        "print('Number of tags: ', len(tags_vect.vocabulary_))\n",
        "\n",
        "tags_token = tags_vect.get_feature_names_out()\n",
        "tag_frequency = tags.sum(axis=0)\n",
        "print('The list of tags with frequency is: ')\n",
        "print(dict(zip(tags_token, tag_frequency)))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pL0HGU975TWq",
        "outputId": "8459877e-cd00-4a06-a42a-1bd8a4716cbb"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The total number of words in the data is:  2195067\n",
            "The number of words in the vocabulary is:  268567\n",
            "Number of tags:  100\n",
            "The list of tags with frequency is: \n",
            "{'.htaccess': 29, '.net': 152, 'actionscript-3': 39, 'ajax': 42, 'algorithm': 65, 'amazon-web-services': 125, 'android': 1405, 'angular': 106, 'angularjs': 163, 'apache': 52, 'apache-spark': 36, 'arrays': 50, 'asp.net': 215, 'asp.net-mvc': 99, 'asp.net-mvc-3': 26, 'assembly': 29, 'azure': 102, 'bash': 87, 'batch-file': 31, 'c': 424, 'c#': 2028, 'c++': 958, 'cakephp': 28, 'css': 265, 'database': 90, 'delphi': 56, 'django': 151, 'docker': 98, 'eclipse': 64, 'elasticsearch': 36, 'excel': 205, 'facebook': 56, 'firebase': 37, 'flutter': 207, 'git': 153, 'go': 72, 'google-app-engine': 28, 'google-apps-script': 38, 'hadoop': 42, 'haskell': 36, 'hibernate': 28, 'html': 485, 'html5': 39, 'ios': 692, 'iphone': 279, 'java': 2324, 'javascript': 2897, 'jquery': 529, 'json': 82, 'kubernetes': 51, 'laravel': 57, 'linux': 192, 'machine-learning': 34, 'matlab': 91, 'maven': 26, 'mongodb': 74, 'mysql': 363, 'next.js': 52, 'nginx': 33, 'node.js': 350, 'objective-c': 171, 'oracle': 50, 'osx': 44, 'performance': 27, 'perl': 58, 'php': 1712, 'postgresql': 82, 'powerbi': 30, 'powershell': 74, 'python': 2267, 'python-3.x': 100, 'qt': 28, 'r': 474, 'react-native': 57, 'reactjs': 300, 'regex': 96, 'ruby': 143, 'ruby-on-rails': 383, 'rust': 58, 'scala': 79, 'shell': 29, 'spring': 79, 'spring-boot': 40, 'sql': 472, 'sql-server': 139, 'swift': 118, 'typescript': 112, 'ubuntu': 33, 'unity-game-engine': 31, 'vb.net': 101, 'vba': 46, 'visual-studio': 44, 'visual-studio-code': 31, 'vue.js': 27, 'wcf': 34, 'windows': 145, 'wordpress': 93, 'wpf': 79, 'xcode': 55, 'xml': 97}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#checking the top most tags based on the frequency\n",
        "\n",
        "tag_count = [[x[0],x[1]] for x in zip(tags_token, tag_frequency)]\n",
        "tag_count = sorted(tag_count, key = lambda x: x[1], reverse=True)\n",
        "most_common_tags=[tag[0] for tag in tag_count]\n",
        "print('Rank     Tag      Count')\n",
        "for i in range(100):\n",
        "    print(\"%d   %s   %d\" % (i+1, tag_count[i][0], tag_count[i][1]))"
      ],
      "metadata": {
        "id": "HdkSV-b_6HSh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5d78def-4ab3-4346-9dfb-5196e4292a4e"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rank     Tag      Count\n",
            "1   javascript   2897\n",
            "2   java   2324\n",
            "3   python   2267\n",
            "4   c#   2028\n",
            "5   php   1712\n",
            "6   android   1405\n",
            "7   c++   958\n",
            "8   ios   692\n",
            "9   jquery   529\n",
            "10   html   485\n",
            "11   r   474\n",
            "12   sql   472\n",
            "13   c   424\n",
            "14   ruby-on-rails   383\n",
            "15   mysql   363\n",
            "16   node.js   350\n",
            "17   reactjs   300\n",
            "18   iphone   279\n",
            "19   css   265\n",
            "20   asp.net   215\n",
            "21   flutter   207\n",
            "22   excel   205\n",
            "23   linux   192\n",
            "24   objective-c   171\n",
            "25   angularjs   163\n",
            "26   git   153\n",
            "27   .net   152\n",
            "28   django   151\n",
            "29   windows   145\n",
            "30   ruby   143\n",
            "31   sql-server   139\n",
            "32   amazon-web-services   125\n",
            "33   swift   118\n",
            "34   typescript   112\n",
            "35   angular   106\n",
            "36   azure   102\n",
            "37   vb.net   101\n",
            "38   python-3.x   100\n",
            "39   asp.net-mvc   99\n",
            "40   docker   98\n",
            "41   xml   97\n",
            "42   regex   96\n",
            "43   wordpress   93\n",
            "44   matlab   91\n",
            "45   database   90\n",
            "46   bash   87\n",
            "47   json   82\n",
            "48   postgresql   82\n",
            "49   scala   79\n",
            "50   spring   79\n",
            "51   wpf   79\n",
            "52   mongodb   74\n",
            "53   powershell   74\n",
            "54   go   72\n",
            "55   algorithm   65\n",
            "56   eclipse   64\n",
            "57   perl   58\n",
            "58   rust   58\n",
            "59   laravel   57\n",
            "60   react-native   57\n",
            "61   delphi   56\n",
            "62   facebook   56\n",
            "63   xcode   55\n",
            "64   apache   52\n",
            "65   next.js   52\n",
            "66   kubernetes   51\n",
            "67   arrays   50\n",
            "68   oracle   50\n",
            "69   vba   46\n",
            "70   osx   44\n",
            "71   visual-studio   44\n",
            "72   ajax   42\n",
            "73   hadoop   42\n",
            "74   spring-boot   40\n",
            "75   actionscript-3   39\n",
            "76   html5   39\n",
            "77   google-apps-script   38\n",
            "78   firebase   37\n",
            "79   apache-spark   36\n",
            "80   elasticsearch   36\n",
            "81   haskell   36\n",
            "82   machine-learning   34\n",
            "83   wcf   34\n",
            "84   nginx   33\n",
            "85   ubuntu   33\n",
            "86   batch-file   31\n",
            "87   unity-game-engine   31\n",
            "88   visual-studio-code   31\n",
            "89   powerbi   30\n",
            "90   .htaccess   29\n",
            "91   assembly   29\n",
            "92   shell   29\n",
            "93   cakephp   28\n",
            "94   google-app-engine   28\n",
            "95   hibernate   28\n",
            "96   qt   28\n",
            "97   performance   27\n",
            "98   vue.js   27\n",
            "99   asp.net-mvc-3   26\n",
            "100   maven   26\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#function to calculate the evaluation metrics\n",
        "\n",
        "def eval_metrics(y_test, y_predicted, print_metrics=True):\n",
        "\n",
        "    accuracy = accuracy_score(y_test, y_predicted)\n",
        "    precision = precision_score(y_test, y_predicted, average='weighted')\n",
        "    recall = recall_score(y_test, y_predicted, average='weighted')\n",
        "    f1 = f1_score(y_test, y_predicted, average='weighted')\n",
        "\n",
        "    if print_metrics:\n",
        "        print(\"f1: %.3f - precision: %.3f - recall: %.3f - accuracy: %.3f\" % (\n",
        "            f1, precision, recall, accuracy))\n",
        "    return f1, precision, recall, accuracy"
      ],
      "metadata": {
        "id": "IeI6PBJw97t4"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.head()\n",
        "\n",
        "train_new=train_data.copy()\n",
        "test_new=test_data.copy()"
      ],
      "metadata": {
        "id": "5Hgi5wKjB9gw"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def tokenize_question(text):\n",
        "    return text.split()\n",
        "\n",
        "def filter_number_features(name):\n",
        "  if name[0] in '0123456789' or len(name)<=3:\n",
        "    return False\n",
        "  return True\n",
        "tfidf_vect = TfidfVectorizer(tokenizer=tokenize_question,\n",
        "                               stop_words='english',\n",
        "                               min_df=4,\n",
        "                               max_df=0.5,max_features=7000)\n",
        "\n",
        "X_train_tfidf = tfidf_vect.fit_transform(train_new[\"Questions\"]).todense()\n",
        "X_test_tfidf = tfidf_vect.transform(test_new[\"Questions\"]).todense()\n",
        "# print('The number of words in the vocabulary is: ', len(tfidf_vect.vocabulary_))\n",
        "\n",
        "\n",
        "\n",
        "#get the feature names\n",
        "feature_names=tfidf_vect.get_feature_names_out()\n",
        "\n",
        "# Get the IDF scores\n",
        "idf_scores = tfidf_vect.idf_\n",
        "\n",
        "Final_Feature_Set=[]\n",
        "for idx,feature_name in enumerate(feature_names):\n",
        "    if filter_number_features(feature_name):\n",
        "       Final_Feature_Set.append([feature_name,idf_scores[idx]])\n",
        "\n",
        "Final_Feature_Set=sorted(Final_Feature_Set,key=lambda x :x[1],reverse=True)\n",
        "Final_Feature_Set=[x[0] for x in Final_Feature_Set]\n",
        "\n",
        "\n",
        "#Another approach (tfidf vectorization as feature set)\n",
        "\n",
        "df_train = pd.DataFrame(X_train_tfidf, columns=tfidf_vect.get_feature_names_out())\n",
        "df_test=pd.DataFrame(X_test_tfidf, columns=tfidf_vect.get_feature_names_out())\n",
        "df_train=df_train[Final_Feature_Set]\n",
        "df_test=df_test[Final_Feature_Set]\n",
        "\n",
        "df_train[\"Tags\"]=train_new[\"Tags\"]\n",
        "df_test[\"Tags\"]=test_new[\"Tags\"]\n"
      ],
      "metadata": {
        "id": "KKLqkfsa-bVI"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_question(text):\n",
        "      return text.split()\n",
        "\n",
        "def CountVectorizer_Custom(data,Features):\n",
        "\n",
        "  questions = data['Questions'].tolist()\n",
        "  tags = data['Tags'].tolist()\n",
        "\n",
        "  question_vect = CountVectorizer(tokenizer=tokenize_question,binary=True,vocabulary=Features)\n",
        "  questions=question_vect.fit_transform(questions)\n",
        "\n",
        "  df_train = pd.DataFrame(questions.toarray(), columns=question_vect.get_feature_names_out())\n",
        "  df_train[\"Tags\"]=tags\n",
        "  return df_train\n",
        "\n",
        "#using count vectorizer as feature set\n",
        "\n",
        "train_new=CountVectorizer_Custom(train_new,Final_Feature_Set)\n",
        "test_new=CountVectorizer_Custom(test_new,Final_Feature_Set)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "__h27EVHE_DC"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_data_by_most_common_tags(data, common_tags):\n",
        "    filtered_data = data[data[\"Tags\"].isin(common_tags)]\n",
        "    return filtered_data\n",
        "\n",
        "\n",
        "def one_hot(column,data): #count vectorizer feature set\n",
        "  # Perform one-hot encoding using get_dummies()\n",
        "  one_hot_encoded = pd.get_dummies(data[column],prefix=\"tag\")\n",
        "\n",
        "  # Concatenate the one-hot encoded columns with the original dataframe\n",
        "  data_extended = pd.concat([data, one_hot_encoded], axis=1)\n",
        "  data_extended.drop(['Tags'],inplace=True,axis=1)\n",
        "\n",
        "  return data_extended\n",
        "\n",
        "def label_encoding(data, most_common_tags):#tfidf vectorizer\n",
        "    v = {}\n",
        "    for j in range(len(most_common_tags)):\n",
        "        v[most_common_tags[j]] = j\n",
        "    data[\"Tags\"] = data[\"Tags\"].apply(lambda x: v[x] if x in v else -1)\n",
        "    return data\n",
        "\n",
        "\n",
        "Final_train=filter_data_by_most_common_tags(df_train,most_common_tags)\n",
        "Final_test=filter_data_by_most_common_tags(df_test,most_common_tags)\n",
        "\n",
        "#dealing with missing tags (count vectorizer)\n",
        "\n",
        "# missing_tags = set(Final_train[\"Tags\"]) - set(Final_test[\"Tags\"])\n",
        "\n",
        "# Final_train=one_hot(\"Tags\",Final_train)\n",
        "\n",
        "# Final_test=one_hot(\"Tags\",Final_test)\n",
        "\n",
        "# for tag in missing_tags:\n",
        "#     Final_test[\"tag_\"+tag] = 0\n",
        "\n",
        "# Final_test=Final_test[Final_train.columns] # in order to maintain order of the columns in test and train\n",
        "\n",
        "\n",
        "\n",
        "#Another Approach (tfidf vectorizer)\n",
        "Final_train=label_encoding(Final_train,most_common_tags)\n",
        "Final_test=label_encoding(Final_test,most_common_tags)\n",
        "\n"
      ],
      "metadata": {
        "id": "FJdoFeOnaMV9"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train=Final_train.iloc[:,:-1]\n",
        "\n",
        "Y_train=Final_train.iloc[:,-1]\n",
        "X_test=Final_test.iloc[:,:-1]\n",
        "\n",
        "Y_test=Final_test.iloc[:,-1]"
      ],
      "metadata": {
        "id": "fEhjmrlXvQAh"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "log_clf = OneVsRestClassifier(LogisticRegression())\n",
        "\n",
        "log_clf.fit(X_train, Y_train)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 117
        },
        "id": "npYTf3n9xBFB",
        "outputId": "2d5e3247-87ee-4143-b7a9-7beaf55f692d"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OneVsRestClassifier(estimator=LogisticRegression())"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>OneVsRestClassifier(estimator=LogisticRegression())</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">OneVsRestClassifier</label><div class=\"sk-toggleable__content\"><pre>OneVsRestClassifier(estimator=LogisticRegression())</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y_train_predict = log_clf.predict(X_train)\n",
        "eval_metrics(Y_train, Y_train_predict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OW_5KuGN5oht",
        "outputId": "07233ae0-e51e-40f1-d511-82b7c1078190"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "f1: 0.818 - precision: 0.952 - recall: 0.721 - accuracy: 0.700\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.8182297686727024, 0.9519304302097099, 0.7212010549807263, 0.699655102454859)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y_test_predict=log_clf.predict(X_test)\n",
        "eval_metrics(Y_test,Y_test_predict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qGYyGZWdxQX5",
        "outputId": "30d0ff23-9d95-4234-897d-73116a75a981"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "f1: 0.383 - precision: 0.639 - recall: 0.290 - accuracy: 0.263\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.38286240616119727,\n",
              " 0.6393702947555568,\n",
              " 0.29014241831890536,\n",
              " 0.2626361351577772)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Classifier - Algorithm - SVM\n",
        "\n",
        "SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
        "SVM.fit(X_train,Y_train)\n",
        "\n",
        "# predict the labels on validation dataset\n",
        "predictions_SVM = SVM.predict(X_test)\n",
        "\n",
        "# Use accuracy_score function to get the accuracy\n",
        "print(\"SVM Accuracy Score -> \",accuracy_score(predictions_SVM, Y_test)*100)"
      ],
      "metadata": {
        "id": "C8TB-KNa7Wzu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NB classifier\n",
        "Naive = naive_bayes.MultinomialNB()\n",
        "\n",
        "Naive.fit(X_train,Y_train)\n",
        "# predict the labels on validation dataset\n",
        "\n",
        "predictions_NB = Naive.predict(X_test)\n",
        "\n",
        "# Use accuracy_score function to get the accuracy\n",
        "\n",
        "print(\"Naive Bayes Accuracy Score -> \",accuracy_score(predictions_NB, Y_test)*100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RvS0elEtfNdt",
        "outputId": "fc558868-a96d-4de4-dcec-a2348a83a350"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Naive Bayes Accuracy Score ->  33.44472291840713\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}