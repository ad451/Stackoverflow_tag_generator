{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ad451/Stackoverflow_tag_generator/blob/main/StackOverflow_Tag_Generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Importing the required modules**"
      ],
      "metadata": {
        "id": "gLtFomOOTrCE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "fy7GyiItTYJx"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import requests\n",
        "import random\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import ToktokTokenizer\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "import string\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Web Scraping the current set of questions for testing the model**"
      ],
      "metadata": {
        "id": "UAaTn6ZCW3CB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "Questions=[] #array to store the questions\n",
        "\n",
        "for pageNumber in range(1,401):\n",
        "    response=requests.get(\"https://stackoverflow.com/questions\",params={\"tab\":\"newest\",\"page\":pageNumber,\"pagesize\":50})\n",
        "\n",
        "    data=BeautifulSoup(response.text,'html.parser' ) #parsing the html text\n",
        "\n",
        "    req_data=data.find(id=\"questions\")\n",
        "\n",
        "    new_data=req_data.find_all(\"h3\", class_=\"s-post-summary--content-title\") # the tag that contains the question info\n",
        "\n",
        "\n",
        "\n",
        "    for element in new_data:\n",
        "        link=element.a.attrs['href']\n",
        "\n",
        "        response=requests.get(f\"https://stackoverflow.com/{link}\")  #fetching the content related to the question\n",
        "\n",
        "        data_questionwise=BeautifulSoup(response.text,'html.parser')\n",
        "\n",
        "        question_wise_title=data_questionwise.find(\"div\",id=\"question-header\").h1.a.string #title\n",
        "\n",
        "        question_wise_desc=data_questionwise.find(\"div\",class_=\"s-prose js-post-body\") #description\n",
        "\n",
        "        all_paragraphs=question_wise_desc.find_all(\"p\")\n",
        "\n",
        "        total_description_question_wise=\"\"\n",
        "\n",
        "        for para in all_paragraphs:\n",
        "            total_description_question_wise+=para.text\n",
        "\n",
        "        Final_content=question_wise_title+\"\"+total_description_question_wise  #concatenating the title and description\n",
        "\n",
        "        tag_question_wise=data_questionwise.find(\"ul\",class_=\"ml0 list-ls-none js-post-tag-list-wrapper d-inline\").li.text #tag\n",
        "\n",
        "        Questions.append([Final_content,tag_question_wise])\n",
        "\n",
        "    print(pageNumber)   #checking which page questions have been fetched yet\n",
        "\n",
        "print(len(Questions))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NKcc1fyLhTBt",
        "outputId": "25598b37-cea6-4af0-80f7-d6990fafa540"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting patool\n",
            "  Downloading patool-1.12-py2.py3-none-any.whl (77 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/77.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.5/77.5 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: patool\n",
            "Successfully installed patool-1.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sql code to combine the input questions and tags table**"
      ],
      "metadata": {
        "id": "D8MnaCTvXb8q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###################################### Code for inner combine ######################################\n",
        "df1 = pd.read_csv('Questions.csv', encoding='ISO-8859-1')\n",
        "df2 = pd.read_csv('Tags.csv', encoding='ISO-8859-1')\n",
        "\n",
        "# combined dataframe of questiontags\n",
        "df3 = df1.set_index('Id').join(df2.set_index('Id'))\n",
        "df3 = df3.reset_index()\n",
        "df4 = pd.DataFrame(df3.Id.value_counts())\n",
        "df4 = df4.reset_index()\n",
        "\n",
        "# df5 is the final combined dataframe of questiontags. Randomly picked datapoints from df3\n",
        "store = {}\n",
        "df5 = pd.DataFrame()\n",
        "n_train = 20000\n",
        "for i in range(n_train):\n",
        "    key = 0\n",
        "    while(key==0):\n",
        "        a = random.randint(df4.shape[0])\n",
        "        if a in store:\n",
        "            continue\n",
        "        else:\n",
        "            key==1\n",
        "            store[a] = 1\n",
        "            ind = df3[df3.Id==df4.iloc[a,0]].first_valid_index()\n",
        "            df5 =  pd.concat([df5,pd.DataFrame(df3.iloc[ind]).T],ignore_index=True)\n",
        "            break\n",
        "\n",
        "###################################### Code for removing angular brackets from the \"Body\" column ######################################\n",
        "\n",
        "for j in range(df5.shape[0]):\n",
        "    ans=\" \"\n",
        "    curr = 0\n",
        "    flag = 0\n",
        "    while(curr<len(df5['Body'][j])):\n",
        "        if( df5['Body'][j][curr]=='<'):\n",
        "            flag = 1\n",
        "            curr = curr + 1\n",
        "            continue\n",
        "        if( df5['Body'][j][curr]=='>'):\n",
        "            flag = 0\n",
        "            curr = curr + 1\n",
        "            continue\n",
        "        if(flag == 1):\n",
        "            curr = curr + 1\n",
        "        if(flag == 0):\n",
        "            ans = ans + df5['Body'][j][curr]\n",
        "            curr = curr + 1\n",
        "    df1['Title'][j] += ans\n",
        "\n",
        "df6 = pd.concat([df5.iloc[:,6], df5.iloc[:,8]], axis=1)"
      ],
      "metadata": {
        "id": "oXOsnSFrjn9b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Machine learning part (Preprocessing and training)**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "kp5z4pMGXlBw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data=pd.read_csv(\"questiontags_train.csv\")\n",
        "test_data=pd.read_csv(\"questiontags_test.csv\")"
      ],
      "metadata": {
        "id": "VKY3pgGoXsJg"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#rename the columns of the train dataset\n",
        "\n",
        "train_data.drop(train_data.columns[0],axis=1,inplace=True)\n",
        "\n",
        "\n",
        "train_data.rename(columns={\"Title\":\"Questions\",\"Tag\":\"Tags\"},inplace=True)\n"
      ],
      "metadata": {
        "id": "CwaW-h2d4PaE"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#code to include around 10000 more questions to the train dataset from the test dataset\n",
        "v={}\n",
        "count=0\n",
        "while count!=10000:\n",
        "    idx=random.randint(0,test_data.shape[0]-1)\n",
        "    if idx not in v:\n",
        "      train_data=train_data.append(test_data.loc[idx],ignore_index=True)\n",
        "      test_data.drop(idx,inplace=True,axis=0)\n",
        "      count+=1\n",
        "      v[idx]=1\n",
        "    else:\n",
        "      continue\n"
      ],
      "metadata": {
        "id": "xZcsg-0j57mm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data['Questions']=train_data['Questions'].str.replace('</p>',' ')\n",
        "train_data['Questions']=train_data['Questions'].str.replace('\\n',' ')\n",
        "train_data['Questions']=train_data['Questions'].str.replace('</a>',' ')\n",
        "train_data['Questions']=train_data['Questions'].apply(lambda x : re.sub('(https?:\\/\\/)?([\\da-z\\.-]+)\\.([a-z\\.]{2,6})([\\/\\w \\.-]*)' , ' ' , x)) #removing any urls\n",
        "train_data['Questions'] = train_data['Questions'].apply(lambda x: ' '.join([w for w in x.split() if len(w) > 3])) #removes small length words (len<3)\n",
        "train_data['Questions']=train_data['Questions'].str.replace('<a href=\" \">','')\n",
        "\n",
        "train_data['Questions']=train_data['Questions'].apply(lambda x : x.lower()) #coverting to lowercase\n"
      ],
      "metadata": {
        "id": "0_RPmQDS7raL"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data['Questions']=test_data['Questions'].str.replace('</p>',' ')\n",
        "test_data['Questions']=test_data['Questions'].str.replace('\\n',' ')\n",
        "test_data['Questions']=test_data['Questions'].str.replace('</a>',' ')\n",
        "test_data['Questions'] = test_data['Questions'].apply(lambda x: ' '.join([w for w in x.split() if len(w) > 3]))\n",
        "test_data['Questions']=test_data['Questions'].str.replace('<a href=\" \">','')\n",
        "test_data['Questions']=test_data['Questions'].apply(lambda x : x.lower())\n",
        "test_data['Questions']=test_data['Questions'].apply(lambda x : re.sub('(https?:\\/\\/)?([\\da-z\\.-]+)\\.([a-z\\.]{2,6})([\\/\\w \\.-]*)' , ' ' , x)) #removing any urls"
      ],
      "metadata": {
        "id": "kGvOPabMHCHF"
      },
      "execution_count": 157,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#removing the stop words and the punctuations from test and train dataset\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stop_words=set(nltk.corpus.stopwords.words('english'))\n",
        "train_data['Questions']=train_data['Questions'].apply(lambda x:' '.join([w for w in x.split() if w not in stop_words]))\n",
        "test_data['Questions']=test_data['Questions'].apply(lambda x:' '.join([w for w in x.split() if w not in stop_words]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G5fomFACHLes",
        "outputId": "e24488d1-37a1-4b76-de87-3b96a2b0d2a5"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#removing the punctuations from test and train dataset\n",
        "\n",
        "\n",
        "punctuations = string.punctuation\n",
        "train_data['Questions']=train_data['Questions'].apply(lambda x:' '.join([''.join([char for char in w if char not in punctuations]) for w in x.split()]))\n",
        "test_data['Questions']=test_data['Questions'].apply(lambda x:' '.join([''.join([char for char in w if char not in punctuations]) for w in x.split()]))\n",
        "\n"
      ],
      "metadata": {
        "id": "QpzdXyd2OPVt"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data=test_data.reset_index()\n"
      ],
      "metadata": {
        "id": "xdBZ0dpiX9n4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "973b3f19-fdd9-4cd7-e28e-323d3f984f0a"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['level_0', 'index', 'Questions', 'Tags'], dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_data.drop(['level_0'],inplace=True,axis=1)"
      ],
      "metadata": {
        "id": "m232NAu4-HE7"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#dropping the rows with empty values of Question after filtering\n",
        "for j in range(len(test_data['Questions'])):\n",
        "  if len(test_data['Questions'][j])==0:\n",
        "     test_data.drop(j,inplace=True)\n",
        "\n",
        "for j in range(len(train_data['Questions'])):\n",
        "  if len(train_data['Questions'][j])==0:\n",
        "     train_data.drop(j,inplace=True)\n",
        "     print(\"yes\")"
      ],
      "metadata": {
        "id": "Vw5EEzCpYLQl"
      },
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data=test_data.reset_index()\n",
        "train_data=train_data.reset_index()"
      ],
      "metadata": {
        "id": "hz8UAEsjaJt2"
      },
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data.drop(['index'],inplace=True,axis=1)\n",
        "train_data.drop(['index'],inplace=True,axis=1)\n"
      ],
      "metadata": {
        "id": "zMtySQj_bfXt"
      },
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z3vqRKOKbk_N",
        "outputId": "066548b5-76a3-489c-d75e-986ce191a269"
      },
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(9176, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 156
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kodAZctqbrBt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}