{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ad451/Stackoverflow_tag_generator/blob/main/StackOverflow_Tag_Generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLtFomOOTrCE"
      },
      "source": [
        "\n",
        "**Importing the required modules**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "fy7GyiItTYJx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2a5ba8f-00ac-47a0-e766-01906bce9020"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import requests\n",
        "import random\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "import warnings; warnings.simplefilter('ignore')\n",
        "import re\n",
        "import string\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import ToktokTokenizer\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk import pos_tag\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils.class_weight import compute_sample_weight\n",
        "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "import gensim\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "\n",
        "nltk.download('averaged_perceptron_tagger') # required for parts of speech\n",
        "nltk.download('wordnet') # required for parts of speech\n",
        "nltk.download('stopwords') #download the stopwords\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAaTn6ZCW3CB"
      },
      "source": [
        "**Web Scraping the current set of questions for testing the model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NKcc1fyLhTBt",
        "outputId": "25598b37-cea6-4af0-80f7-d6990fafa540"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting patool\n",
            "  Downloading patool-1.12-py2.py3-none-any.whl (77 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/77.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.5/77.5 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: patool\n",
            "Successfully installed patool-1.12\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "Questions=[] #array to store the questions\n",
        "\n",
        "for pageNumber in range(1,401):\n",
        "    response=requests.get(\"https://stackoverflow.com/questions\",params={\"tab\":\"newest\",\"page\":pageNumber,\"pagesize\":50})\n",
        "\n",
        "    data=BeautifulSoup(response.text,'html.parser' ) #parsing the html text\n",
        "\n",
        "    req_data=data.find(id=\"questions\")\n",
        "\n",
        "    new_data=req_data.find_all(\"h3\", class_=\"s-post-summary--content-title\") # the tag that contains the question info\n",
        "\n",
        "\n",
        "\n",
        "    for element in new_data:\n",
        "        link=element.a.attrs['href']\n",
        "\n",
        "        response=requests.get(f\"https://stackoverflow.com/{link}\")  #fetching the content related to the question\n",
        "\n",
        "        data_questionwise=BeautifulSoup(response.text,'html.parser')\n",
        "\n",
        "        question_wise_title=data_questionwise.find(\"div\",id=\"question-header\").h1.a.string #title\n",
        "\n",
        "        question_wise_desc=data_questionwise.find(\"div\",class_=\"s-prose js-post-body\") #description\n",
        "\n",
        "        all_paragraphs=question_wise_desc.find_all(\"p\")\n",
        "\n",
        "        total_description_question_wise=\"\"\n",
        "\n",
        "        for para in all_paragraphs:\n",
        "            total_description_question_wise+=para.text\n",
        "\n",
        "        Final_content=question_wise_title+\"\"+total_description_question_wise  #concatenating the title and description\n",
        "\n",
        "        tag_question_wise=data_questionwise.find(\"ul\",class_=\"ml0 list-ls-none js-post-tag-list-wrapper d-inline\").li.text #tag\n",
        "\n",
        "        Questions.append([Final_content,tag_question_wise])\n",
        "\n",
        "    print(pageNumber)   #checking which page questions have been fetched yet\n",
        "\n",
        "print(len(Questions))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8MnaCTvXb8q"
      },
      "source": [
        "**Sql code to combine the input questions and tags table**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oXOsnSFrjn9b"
      },
      "outputs": [],
      "source": [
        "###################################### Code for inner combine ######################################\n",
        "df1 = pd.read_csv('Questions.csv', encoding='ISO-8859-1')\n",
        "df2 = pd.read_csv('Tags.csv', encoding='ISO-8859-1')\n",
        "\n",
        "# combined dataframe of questiontags\n",
        "df3 = df1.set_index('Id').join(df2.set_index('Id'))\n",
        "df3 = df3.reset_index()\n",
        "df4 = pd.DataFrame(df3.Id.value_counts())\n",
        "df4 = df4.reset_index()\n",
        "\n",
        "# df5 is the final combined dataframe of questiontags. Randomly picked datapoints from df3\n",
        "store = {}\n",
        "df5 = pd.DataFrame()\n",
        "n_train = 20000\n",
        "for i in range(n_train):\n",
        "    key = 0\n",
        "    while(key==0):\n",
        "        a = random.randint(df4.shape[0])\n",
        "        if a in store:\n",
        "            continue\n",
        "        else:\n",
        "            key==1\n",
        "            store[a] = 1\n",
        "            ind = df3[df3.Id==df4.iloc[a,0]].first_valid_index()\n",
        "            df5 =  pd.concat([df5,pd.DataFrame(df3.iloc[ind]).T],ignore_index=True)\n",
        "            break\n",
        "\n",
        "###################################### Code for removing angular brackets from the \"Body\" column ######################################\n",
        "\n",
        "for j in range(df5.shape[0]):\n",
        "    ans=\" \"\n",
        "    curr = 0\n",
        "    flag = 0\n",
        "    while(curr<len(df5['Body'][j])):\n",
        "        if( df5['Body'][j][curr]=='<'):\n",
        "            flag = 1\n",
        "            curr = curr + 1\n",
        "            continue\n",
        "        if( df5['Body'][j][curr]=='>'):\n",
        "            flag = 0\n",
        "            curr = curr + 1\n",
        "            continue\n",
        "        if(flag == 1):\n",
        "            curr = curr + 1\n",
        "        if(flag == 0):\n",
        "            ans = ans + df5['Body'][j][curr]\n",
        "            curr = curr + 1\n",
        "    df1['Title'][j] += ans\n",
        "\n",
        "df6 = pd.concat([df5.iloc[:,6], df5.iloc[:,8]], axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kp5z4pMGXlBw"
      },
      "source": [
        "**Machine learning part (Preprocessing and training)**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VKY3pgGoXsJg"
      },
      "outputs": [],
      "source": [
        "train_data=pd.read_csv(\"questiontags_train.csv\")\n",
        "test_data=pd.read_csv(\"questiontags_test.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CwaW-h2d4PaE"
      },
      "outputs": [],
      "source": [
        "#rename the columns of the train dataset\n",
        "\n",
        "train_data.drop(train_data.columns[0],axis=1,inplace=True)\n",
        "\n",
        "\n",
        "train_data.rename(columns={\"Title\":\"Questions\",\"Tag\":\"Tags\"},inplace=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xZcsg-0j57mm"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "#code to include around 10000 more questions to the train dataset from the test dataset\n",
        "v={}\n",
        "count=0\n",
        "while count!=10000:\n",
        "    idx=random.randint(0,test_data.shape[0]-1)\n",
        "    if idx not in v:\n",
        "      train_data=train_data.append(test_data.loc[idx],ignore_index=True)\n",
        "      test_data.drop(idx,inplace=True,axis=0)\n",
        "      count+=1\n",
        "      v[idx]=1\n",
        "    else:\n",
        "      continue\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0_RPmQDS7raL"
      },
      "outputs": [],
      "source": [
        "train_data['Questions']=train_data['Questions'].str.replace('</p>',' ')\n",
        "train_data['Questions']=train_data['Questions'].str.replace('\\n',' ')\n",
        "train_data['Questions']=train_data['Questions'].str.replace('</a>',' ')\n",
        "train_data['Questions']=train_data['Questions'].apply(lambda x : re.sub('(https?:\\/\\/)?([\\da-z\\.-]+)\\.([a-z\\.]{2,6})([\\/\\w \\.-]*)' , ' ' , x)) #removing any urls\n",
        "train_data['Questions'] = train_data['Questions'].apply(lambda x: ' '.join([w for w in x.split() if len(w) > 3])) #removes small length words (len<3)\n",
        "train_data['Questions']=train_data['Questions'].str.replace('<a href=\" \">','')\n",
        "\n",
        "train_data['Questions']=train_data['Questions'].apply(lambda x : x.lower()) #coverting to lowercase\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kGvOPabMHCHF"
      },
      "outputs": [],
      "source": [
        "test_data['Questions']=test_data['Questions'].str.replace('</p>',' ')\n",
        "test_data['Questions']=test_data['Questions'].str.replace('\\n',' ')\n",
        "test_data['Questions']=test_data['Questions'].str.replace('</a>',' ')\n",
        "test_data['Questions'] = test_data['Questions'].apply(lambda x: ' '.join([w for w in x.split() if len(w) > 3]))\n",
        "test_data['Questions']=test_data['Questions'].str.replace('<a href=\" \">','')\n",
        "test_data['Questions']=test_data['Questions'].apply(lambda x : x.lower())\n",
        "test_data['Questions']=test_data['Questions'].apply(lambda x : re.sub('(https?:\\/\\/)?([\\da-z\\.-]+)\\.([a-z\\.]{2,6})([\\/\\w \\.-]*)' , ' ' , x)) #removing any urls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G5fomFACHLes"
      },
      "outputs": [],
      "source": [
        "#removing the stop words and the punctuations from test and train dataset\n",
        "\n",
        "\n",
        "stop_words=set(nltk.corpus.stopwords.words('english'))\n",
        "train_data['Questions']=train_data['Questions'].apply(lambda x:' '.join([w for w in x.split() if w not in stop_words]))\n",
        "test_data['Questions']=test_data['Questions'].apply(lambda x:' '.join([w for w in x.split() if w not in stop_words]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QpzdXyd2OPVt"
      },
      "outputs": [],
      "source": [
        "#removing the punctuations from test and train dataset\n",
        "\n",
        "\n",
        "punctuations = string.punctuation\n",
        "train_data['Questions']=train_data['Questions'].apply(lambda x:' '.join([''.join([char for char in w if char not in punctuations]) for w in x.split()]))\n",
        "test_data['Questions']=test_data['Questions'].apply(lambda x:' '.join([''.join([char for char in w if char not in punctuations]) for w in x.split()]))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xdBZ0dpiX9n4"
      },
      "outputs": [],
      "source": [
        "test_data=test_data.reset_index()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m232NAu4-HE7"
      },
      "outputs": [],
      "source": [
        "\n",
        "test_data.drop(['index'],inplace=True,axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vw5EEzCpYLQl",
        "outputId": "9181714a-5d01-4a08-dd54-3ee98ca6736a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "yes\n",
            "yes\n",
            "yes\n",
            "yes\n",
            "yes\n"
          ]
        }
      ],
      "source": [
        "#dropping the rows with empty values of Question after filtering\n",
        "for j in range(len(test_data['Questions'])):\n",
        "  if len(test_data['Questions'][j])==0:\n",
        "     test_data.drop(j,inplace=True)\n",
        "\n",
        "for j in range(len(train_data['Questions'])):\n",
        "  if len(train_data['Questions'][j])==0:\n",
        "     train_data.drop(j,inplace=True)\n",
        "     print(\"yes\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hz8UAEsjaJt2"
      },
      "outputs": [],
      "source": [
        "test_data=test_data.reset_index()\n",
        "train_data=train_data.reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zMtySQj_bfXt"
      },
      "outputs": [],
      "source": [
        "test_data.drop(['index'],inplace=True,axis=1)\n",
        "train_data.drop(['index'],inplace=True,axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z3vqRKOKbk_N"
      },
      "outputs": [],
      "source": [
        "#using lemmatization on the questions of the train and test dataset\n",
        "def lemmatization(text):\n",
        "    pos_dict = {\n",
        "        'N': 'n',  # Noun\n",
        "        'V': 'v',  # Verb\n",
        "        'R': 'r',  # Adverb\n",
        "        'J': 'a'   # Adjective\n",
        "    }\n",
        "    pos_tags = pos_tag(text)\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemma=[]\n",
        "    for word, tag in pos_tags:\n",
        "        if (tag[0].upper() not in pos_dict.keys()):\n",
        "          pos='n'\n",
        "        else:\n",
        "          pos= pos_dict[tag[0].upper()]\n",
        "        lemma.append(lemmatizer.lemmatize(word,pos=pos))\n",
        "    return lemma\n",
        "\n",
        "train_data['Questions']=train_data['Questions'].apply(lambda x : lemmatization(x.split()))\n",
        "test_data['Questions']=test_data['Questions'].apply(lambda x : lemmatization(x.split()))\n",
        "train_data[\"Questions\"]=train_data[\"Questions\"].apply(lambda x : \" \".join(x))\n",
        "\n",
        "test_data[\"Questions\"]=test_data[\"Questions\"].apply(lambda x : \" \".join(x))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Exploration with dev of Base Model**"
      ],
      "metadata": {
        "id": "MKRKercrIKUn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "questions = train_data['Questions'].tolist()\n",
        "tags = train_data['Tags'].tolist()\n",
        "\n",
        "print('The total number of words in the data is: ', sum([len(text.split()) for text in questions]))\n",
        "\n",
        "def tokenize_question(text):\n",
        "    return text.split()\n",
        "\n",
        "question_vect = CountVectorizer(tokenizer=tokenize_question)\n",
        "questions=question_vect.fit_transform(questions)\n",
        "\n",
        "print('The number of words in the vocabulary is: ', len(question_vect.vocabulary_))\n",
        "\n",
        "def tokenize_tags(text):\n",
        "    return text.split('|')\n",
        "\n",
        "max_tags = 100\n",
        "tags_vect = CountVectorizer(tokenizer=tokenize_tags, max_features=max_tags)\n",
        "tags = tags_vect.fit_transform(tags)\n",
        "tags = tags.toarray()\n",
        "print('Number of tags: ', len(tags_vect.vocabulary_))\n",
        "\n",
        "tags_token = tags_vect.get_feature_names_out()\n",
        "tag_frequency = tags.sum(axis=0)\n",
        "print('The list of tags with frequency is: ')\n",
        "print(dict(zip(tags_token, tag_frequency)))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pL0HGU975TWq",
        "outputId": "f04adae2-a8d2-46d1-a1d5-9bbb86bf1d37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The total number of words in the data is:  2201637\n",
            "The number of words in the vocabulary is:  269815\n",
            "Number of tags:  100\n",
            "The list of tags with frequency is: \n",
            "{'.htaccess': 30, '.net': 156, 'actionscript-3': 39, 'ajax': 42, 'algorithm': 68, 'amazon-web-services': 125, 'android': 1402, 'angular': 106, 'angularjs': 163, 'apache': 52, 'apache-spark': 40, 'arrays': 53, 'asp.net': 216, 'asp.net-mvc': 102, 'assembly': 27, 'azure': 109, 'bash': 77, 'batch-file': 32, 'c': 402, 'c#': 2015, 'c++': 991, 'cakephp': 28, 'css': 263, 'database': 84, 'delphi': 53, 'django': 145, 'docker': 109, 'eclipse': 65, 'elasticsearch': 35, 'excel': 208, 'facebook': 53, 'firebase': 34, 'flutter': 212, 'git': 152, 'github': 30, 'go': 67, 'google-apps-script': 40, 'hadoop': 42, 'haskell': 35, 'html': 488, 'html5': 39, 'ios': 674, 'iphone': 279, 'java': 2350, 'javascript': 2883, 'jquery': 530, 'jsf': 27, 'json': 90, 'kubernetes': 49, 'laravel': 57, 'linux': 201, 'machine-learning': 30, 'magento': 29, 'matlab': 87, 'maven': 26, 'mongodb': 74, 'ms-access': 26, 'mysql': 368, 'next.js': 42, 'node.js': 333, 'objective-c': 172, 'oracle': 47, 'osx': 44, 'performance': 29, 'perl': 59, 'php': 1705, 'postgresql': 90, 'powerbi': 32, 'powershell': 78, 'python': 2322, 'python-3.x': 104, 'qt': 29, 'r': 472, 'react-native': 57, 'reactjs': 297, 'regex': 99, 'rest': 26, 'ruby': 143, 'ruby-on-rails': 389, 'rust': 52, 'scala': 81, 'shell': 32, 'spring': 83, 'spring-boot': 44, 'sql': 469, 'sql-server': 137, 'swift': 122, 'typescript': 103, 'ubuntu': 35, 'unity-game-engine': 28, 'vb.net': 95, 'vba': 43, 'visual-studio': 46, 'vue.js': 29, 'wcf': 34, 'windows': 141, 'wordpress': 97, 'wpf': 83, 'xcode': 54, 'xml': 96}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#checking the top most tags based on the frequency\n",
        "\n",
        "tag_count = [[x[0],x[1]] for x in zip(tags_token, tag_frequency)]\n",
        "tag_count = sorted(tag_count, key = lambda x: x[1], reverse=True)\n",
        "most_common_tags=[tag[0] for tag in tag_count]\n",
        "print('Rank     Tag      Count')\n",
        "for i in range(100):\n",
        "    print(\"%d   %s   %d\" % (i+1, tag_count[i][0], tag_count[i][1]))"
      ],
      "metadata": {
        "id": "HdkSV-b_6HSh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6a65f86-4bba-4a86-d85d-f73e2b376a07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rank     Tag      Count\n",
            "1   javascript   2883\n",
            "2   java   2350\n",
            "3   python   2322\n",
            "4   c#   2015\n",
            "5   php   1705\n",
            "6   android   1402\n",
            "7   c++   991\n",
            "8   ios   674\n",
            "9   jquery   530\n",
            "10   html   488\n",
            "11   r   472\n",
            "12   sql   469\n",
            "13   c   402\n",
            "14   ruby-on-rails   389\n",
            "15   mysql   368\n",
            "16   node.js   333\n",
            "17   reactjs   297\n",
            "18   iphone   279\n",
            "19   css   263\n",
            "20   asp.net   216\n",
            "21   flutter   212\n",
            "22   excel   208\n",
            "23   linux   201\n",
            "24   objective-c   172\n",
            "25   angularjs   163\n",
            "26   .net   156\n",
            "27   git   152\n",
            "28   django   145\n",
            "29   ruby   143\n",
            "30   windows   141\n",
            "31   sql-server   137\n",
            "32   amazon-web-services   125\n",
            "33   swift   122\n",
            "34   azure   109\n",
            "35   docker   109\n",
            "36   angular   106\n",
            "37   python-3.x   104\n",
            "38   typescript   103\n",
            "39   asp.net-mvc   102\n",
            "40   regex   99\n",
            "41   wordpress   97\n",
            "42   xml   96\n",
            "43   vb.net   95\n",
            "44   json   90\n",
            "45   postgresql   90\n",
            "46   matlab   87\n",
            "47   database   84\n",
            "48   spring   83\n",
            "49   wpf   83\n",
            "50   scala   81\n",
            "51   powershell   78\n",
            "52   bash   77\n",
            "53   mongodb   74\n",
            "54   algorithm   68\n",
            "55   go   67\n",
            "56   eclipse   65\n",
            "57   perl   59\n",
            "58   laravel   57\n",
            "59   react-native   57\n",
            "60   xcode   54\n",
            "61   arrays   53\n",
            "62   delphi   53\n",
            "63   facebook   53\n",
            "64   apache   52\n",
            "65   rust   52\n",
            "66   kubernetes   49\n",
            "67   oracle   47\n",
            "68   visual-studio   46\n",
            "69   osx   44\n",
            "70   spring-boot   44\n",
            "71   vba   43\n",
            "72   ajax   42\n",
            "73   hadoop   42\n",
            "74   next.js   42\n",
            "75   apache-spark   40\n",
            "76   google-apps-script   40\n",
            "77   actionscript-3   39\n",
            "78   html5   39\n",
            "79   elasticsearch   35\n",
            "80   haskell   35\n",
            "81   ubuntu   35\n",
            "82   firebase   34\n",
            "83   wcf   34\n",
            "84   batch-file   32\n",
            "85   powerbi   32\n",
            "86   shell   32\n",
            "87   .htaccess   30\n",
            "88   github   30\n",
            "89   machine-learning   30\n",
            "90   magento   29\n",
            "91   performance   29\n",
            "92   qt   29\n",
            "93   vue.js   29\n",
            "94   cakephp   28\n",
            "95   unity-game-engine   28\n",
            "96   assembly   27\n",
            "97   jsf   27\n",
            "98   maven   26\n",
            "99   ms-access   26\n",
            "100   rest   26\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# X_test = test_data['Questions'].tolist()\n",
        "# y_test = test_data['Tags'].tolist()"
      ],
      "metadata": {
        "id": "0EGEl52m9p-J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# X_train, X_test, y_train, y_test = train_test_split(questions, tags,\n",
        "#                                                     test_size=0.2,\n",
        "#                                                     random_state=747)\n",
        "\n",
        "# X_val, X_test, y_val, y_test = train_test_split(X_test, y_test,\n",
        "#                                                     test_size=0.5,\n",
        "#                                                     random_state=747)"
      ],
      "metadata": {
        "id": "WADg9CVe8CX6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#function to calculate the evaluation metrics\n",
        "\n",
        "def eval_metrics(y_test, y_predicted, print_metrics=True):\n",
        "\n",
        "    accuracy = accuracy_score(y_test, y_predicted)\n",
        "    precision = precision_score(y_test, y_predicted, average='weighted')\n",
        "    recall = recall_score(y_test, y_predicted, average='weighted')\n",
        "    f1 = f1_score(y_test, y_predicted, average='weighted')\n",
        "\n",
        "    if print_metrics:\n",
        "        print(\"f1: %.3f - precision: %.3f - recall: %.3f - accuracy: %.3f\" % (\n",
        "            f1, precision, recall, accuracy))\n",
        "    return f1, precision, recall, accuracy"
      ],
      "metadata": {
        "id": "IeI6PBJw97t4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.head()\n",
        "\n",
        "train_new=train_data.copy()\n",
        "test_new=test_data.copy()"
      ],
      "metadata": {
        "id": "5Hgi5wKjB9gw"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def tokenize_question(text):\n",
        "    return text.split()\n",
        "\n",
        "def filter_number_features(name):\n",
        "  if name[0] in '0123456789' or len(name)<=3:\n",
        "    return False\n",
        "  return True\n",
        "tfidf_vect = TfidfVectorizer(tokenizer=tokenize_question,\n",
        "                               stop_words='english',\n",
        "                               min_df=4,\n",
        "                               max_df=0.5,max_features=7000)\n",
        "\n",
        "X_train_tfidf = tfidf_vect.fit_transform(train_new[\"Questions\"]).todense()\n",
        "X_test_tfidf = tfidf_vect.transform(test_new[\"Questions\"]).todense()\n",
        "# print('The number of words in the vocabulary is: ', len(tfidf_vect.vocabulary_))\n",
        "\n",
        "\n",
        "\n",
        "#get the feature names\n",
        "feature_names=tfidf_vect.get_feature_names_out()\n",
        "\n",
        "# Get the IDF scores\n",
        "idf_scores = tfidf_vect.idf_\n",
        "\n",
        "Final_Feature_Set=[]\n",
        "for idx,feature_name in enumerate(feature_names):\n",
        "    if filter_number_features(feature_name):\n",
        "       Final_Feature_Set.append([feature_name,idf_scores[idx]])\n",
        "\n",
        "Final_Feature_Set=sorted(Final_Feature_Set,key=lambda x :x[1],reverse=True)\n",
        "Final_Feature_Set=[x[0] for x in Final_Feature_Set]\n"
      ],
      "metadata": {
        "id": "KKLqkfsa-bVI"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_question(text):\n",
        "      return text.split()\n",
        "\n",
        "def CountVectorizer_Custom(data,Features):\n",
        "\n",
        "  questions = data['Questions'].tolist()\n",
        "  tags = data['Tags'].tolist()\n",
        "\n",
        "  question_vect = CountVectorizer(tokenizer=tokenize_question,binary=True,vocabulary=Features)\n",
        "  questions=question_vect.fit_transform(questions)\n",
        "\n",
        "  df_train = pd.DataFrame(questions.toarray(), columns=question_vect.get_feature_names_out())\n",
        "  df_train[\"Tags\"]=tags\n",
        "  return df_train\n",
        "\n",
        "train_new=CountVectorizer_Custom(train_new,Final_Feature_Set)\n",
        "test_new=CountVectorizer_Custom(test_new,Final_Feature_Set)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "__h27EVHE_DC"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_data_by_most_common_tags(data, common_tags):\n",
        "    filtered_data = data[data[\"Tags\"].isin(common_tags)]\n",
        "    return filtered_data\n",
        "\n",
        "\n",
        "def one_hot(column,data):\n",
        "  # Perform one-hot encoding using get_dummies()\n",
        "  one_hot_encoded = pd.get_dummies(data[column],prefix=\"tag\")\n",
        "\n",
        "  # Concatenate the one-hot encoded columns with the original dataframe\n",
        "  data_extended = pd.concat([data, one_hot_encoded], axis=1)\n",
        "  data_extended.drop(['Tags'],inplace=True,axis=1)\n",
        "\n",
        "  return data_extended\n",
        "\n",
        "\n",
        "Final_train=filter_data_by_most_common_tags(train_new,most_common_tags)\n",
        "Final_test=filter_data_by_most_common_tags(test_new,most_common_tags)\n",
        "\n",
        "#dealing with missing tags\n",
        "\n",
        "missing_tags = set(Final_train[\"Tags\"]) - set(Final_test[\"Tags\"])\n",
        "\n",
        "Final_train=one_hot(\"Tags\",Final_train)\n",
        "\n",
        "Final_test=one_hot(\"Tags\",Final_test)\n",
        "\n",
        "for tag in missing_tags:\n",
        "    Final_test[\"tag_\"+tag] = 0\n",
        "\n",
        "Final_test=Final_test[Final_train.columns] # in order to maintain order of the columns in test and train\n"
      ],
      "metadata": {
        "id": "FJdoFeOnaMV9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train=Final_train.iloc[:,:-1*len(most_common_tags)]\n",
        "\n",
        "Y_train=Final_train.iloc[:,-1*len(most_common_tags):]\n",
        "X_test=Final_test.iloc[:,:-1*len(most_common_tags)]\n",
        "\n",
        "Y_test=Final_test.iloc[:,-1*len(most_common_tags):]"
      ],
      "metadata": {
        "id": "fEhjmrlXvQAh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "tfidf_log_clf = OneVsRestClassifier(LogisticRegression())\n",
        "\n",
        "tfidf_log_clf.fit(X_train_tfidf, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "id": "Vj1D6BPbG0Hv",
        "outputId": "ddd2fdd1-1679-49d3-ae71-dcba17d10385"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 27.6 s, sys: 28.5 s, total: 56.1 s\n",
            "Wall time: 32 s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OneVsRestClassifier(estimator=LogisticRegression())"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>OneVsRestClassifier(estimator=LogisticRegression())</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">OneVsRestClassifier</label><div class=\"sk-toggleable__content\"><pre>OneVsRestClassifier(estimator=LogisticRegression())</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_tfidf_predict = tfidf_log_clf.predict(X_train_tfidf)\n",
        "eval_metrics(y_train, y_train_tfidf_predict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eoa36tt0HFSH",
        "outputId": "0af1e221-7738-4296-f7c2-4c974d524a21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "f1: 0.287 - precision: 0.705 - recall: 0.192 - accuracy: 0.325\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.2869979496204687,\n",
              " 0.7048346764490622,\n",
              " 0.19239736441966548,\n",
              " 0.3247759949989581)"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_test_tfidf_predict = tfidf_log_clf.predict(X_test_tfidf)\n",
        "eval_metrics(y_test, y_test_tfidf_predict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VaPKwJwDHP8P",
        "outputId": "8f014bc0-27e6-4cf6-ef11-110db577d36f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "f1: 0.246 - precision: 0.563 - recall: 0.164 - accuracy: 0.287\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.24568413711051668, 0.5629343082208758, 0.1643342662934826, 0.287)"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_log_clf = OneVsRestClassifier(LogisticRegression())\n",
        "\n",
        "tfidf_log_clf.fit(X_train, Y_train)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "id": "npYTf3n9xBFB",
        "outputId": "bd016507-05bc-49f6-cfb7-121d2d522bd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-68bc91dbb806>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtfidf_log_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mY_train_predict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfidf_log_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_tfidf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0meval_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_tfidf_predict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/multiclass.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    445\u001b[0m             \u001b[0mindptr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"i\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 447\u001b[0;31m                 \u001b[0mindices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_predict_binary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mthresh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    448\u001b[0m                 \u001b[0mindptr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/multiclass.py\u001b[0m in \u001b[0;36m_predict_binary\u001b[0;34m(estimator, X)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mAttributeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;31m# probabilities of the positive class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_base.py\u001b[0m in \u001b[0;36mdecision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    398\u001b[0m         \u001b[0mxp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_namespace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"csr\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    401\u001b[0m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdense_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintercept_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Validation should be done on X, y or both.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 565\u001b[0;31m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"X\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    566\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    735\u001b[0m     \"\"\"\n\u001b[1;32m    736\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 737\u001b[0;31m         raise TypeError(\n\u001b[0m\u001b[1;32m    738\u001b[0m             \u001b[0;34m\"np.matrix is not supported. Please convert to a numpy array with \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m             \u001b[0;34m\"np.asarray. For more information see: \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: np.matrix is not supported. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y_train_predict = tfidf_log_clf.predict(X_train)\n",
        "eval_metrics(Y_train, Y_train_predict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OW_5KuGN5oht",
        "outputId": "55fc4047-4443-4d3c-d9ef-1fe7e77f4055"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "f1: 0.817 - precision: 0.953 - recall: 0.719 - accuracy: 0.697\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.816908226438034, 0.9532802106957978, 0.7186436558094312, 0.6970507211148922)"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y_test_predict=tfidf_log_clf.predict(X_test)\n",
        "eval_metrics(Y_test,Y_test_predict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "qGYyGZWdxQX5",
        "outputId": "61158c32-4760-4b7a-c2a0-dd19e2736be6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-7567bb1d83fc>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mY_test_predict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtfidf_log_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0meval_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_test_predict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/multiclass.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    445\u001b[0m             \u001b[0mindptr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"i\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 447\u001b[0;31m                 \u001b[0mindices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_predict_binary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mthresh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    448\u001b[0m                 \u001b[0mindptr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/multiclass.py\u001b[0m in \u001b[0;36m_predict_binary\u001b[0;34m(estimator, X)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mAttributeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;31m# probabilities of the positive class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_base.py\u001b[0m in \u001b[0;36mdecision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    398\u001b[0m         \u001b[0mxp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_namespace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"csr\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    401\u001b[0m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdense_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintercept_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    546\u001b[0m             \u001b[0mvalidated\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m         \"\"\"\n\u001b[0;32m--> 548\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    549\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"requires_y\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_check_feature_names\u001b[0;34m(self, X, reset)\u001b[0m\n\u001b[1;32m    479\u001b[0m                 )\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 481\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    482\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m     def _validate_data(\n",
            "\u001b[0;31mValueError\u001b[0m: The feature names should match those that were passed during fit.\nFeature names seen at fit time, yet now missing:\n- code\n- error\n- like\n- need\n- want\n- ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "column1=X_train.columns\n",
        "column2=X_test.columns\n",
        "\n",
        "print(len(column1))\n",
        "print(len(column2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C8TB-KNa7Wzu",
        "outputId": "e0ddde07-75ff-479c-f995-ffb3be3b9f9f"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6137\n",
            "6137\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dWT79nGa7yEU"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}