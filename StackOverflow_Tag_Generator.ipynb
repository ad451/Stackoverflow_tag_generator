{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gLtFomOOTrCE"
   },
   "source": [
    "\n",
    "**Importing the required modules**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fy7GyiItTYJx",
    "outputId": "94541196-7d36-42b8-f402-dc33960c4a86"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import random\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "# from google.colab import files\n",
    "\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "import gensim\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score,jaccard_score\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler #fixed import\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense,Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger') # required for parts of speech\n",
    "nltk.download('wordnet') # required for parts of speech\n",
    "nltk.download('stopwords') #download the stopwords\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D8MnaCTvXb8q"
   },
   "source": [
    "**combine the questions and tags table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oXOsnSFrjn9b"
   },
   "outputs": [],
   "source": [
    "###################################### Code for inner combine ######################################\n",
    "df1 = pd.read_csv('Questions.csv', encoding='ISO-8859-1')\n",
    "df2 = pd.read_csv('Tags.csv', encoding='ISO-8859-1')\n",
    "\n",
    "# combined dataframe of questiontags\n",
    "df3 = df1.set_index('Id').join(df2.set_index('Id'))\n",
    "df3=shuffle(df3)\n",
    "df3 = df3.reset_index()\n",
    "\n",
    "\n",
    "\n",
    "###################################### Code for preparing the train_data from the total data ######################################\n",
    "\n",
    "\n",
    "#only taking questions with score greater than or equal to 3\n",
    "\n",
    "df4=df3[df3[\"Score\"]>=1]\n",
    "\n",
    "#generating the list of all unique question ids and also for ranking tags based on their popularity\n",
    "\n",
    "unique_ids=Counter(df4[\"Id\"])\n",
    "q=sorted(zip(Counter(df4[\"Tag\"]).values(),Counter(df4[\"Tag\"]).keys()),reverse=True)\n",
    "rank={}\n",
    "for j in range(len(q)):\n",
    "    rank[q[j][1]]=j+1\n",
    "    \n",
    "    \n",
    "keys=list(unique_ids.keys())\n",
    "\n",
    "# iterating over each unique question id and assigning only one tag to that based on the ranking of the tag\n",
    "Tags=Counter(df4[\"Tag\"])\n",
    "Final_dataframe={\"Body\":[],\"Title\":[],\"Tags\":[]}\n",
    "for key in keys:\n",
    "    current_df=df4[df4[\"Id\"]==key]\n",
    "    selected_tag=-1\n",
    "    selected_tag_rank=-1\n",
    "    Body=list(current_df[\"Body\"])[0]\n",
    "    Title=list(current_df[\"Title\"])[0]\n",
    "    \n",
    "    for tag in current_df[\"Tag\"]:\n",
    "        if rank[tag]>selected_tag_rank:\n",
    "            selected_tag=tag\n",
    "            selected_tag_rank=rank[tag]\n",
    "    Final_dataframe[\"Body\"].append(Body)\n",
    "    Final_dataframe[\"Title\"].append(Title)\n",
    "    Final_dataframe[\"Tags\"].append(selected_tag)\n",
    "\n",
    "df5=pd.DataFrame(Final_dataframe)\n",
    "\n",
    "#concatenating the title and the body columns into the Questions column\n",
    "df5[\"Questions\"]=df5[\"Title\"]+\" \"+df5[\"Body\"]\n",
    "\n",
    "df5.drop([\"Body\",\"Title\"],axis=1,inplace=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=df5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=pd.read_csv(\"New_data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kp5z4pMGXlBw"
   },
   "source": [
    "**Machine learning part (Preprocessing and exploration)**\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping rows with nan values in Tags column\n",
    "\n",
    "train_data.dropna(subset=['Tags'], inplace=True)\n",
    "train_data=train_data.reset_index(drop=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0_RPmQDS7raL"
   },
   "outputs": [],
   "source": [
    "#data cleaning \n",
    "substrings_to_replace = ['</p>', '<p>','\\n','<pre>','</pre>','<a href=\" \">']\n",
    "for substring in substrings_to_replace:\n",
    "    train_data['Questions'] = train_data['Questions'].str.replace(substring, ' ')\n",
    "\n",
    "\n",
    "train_data['Questions']=train_data['Questions'].apply(lambda x : re.sub(r'<code>.*?</code>', ' ', x, flags=re.DOTALL)) #removing any urls\n",
    "\n",
    "train_data['Questions']=train_data['Questions'].apply(lambda x : re.sub('(https?:\\/\\/)?([\\da-z\\.-]+)\\.([a-z\\.]{2,6})([\\/\\w \\.-]*)' , ' ' , x)) #removing any urls\n",
    "train_data['Questions'] = train_data['Questions'].apply(lambda x: ' '.join([w for w in x.split() if len(w) > 3])) #removes small length words (len<3)\n",
    "\n",
    "train_data['Questions']=train_data['Questions'].apply(lambda x : x.lower()) #coverting to lowercase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G5fomFACHLes"
   },
   "outputs": [],
   "source": [
    "#removing the stop words and the punctuations from test and train dataset\n",
    "\n",
    "punctuations = string.punctuation\n",
    "\n",
    "stop_words=set(nltk.corpus.stopwords.words('english'))\n",
    "train_data['Questions']=train_data['Questions'].apply(lambda x:' '.join([w for w in x.split() if w not in stop_words]))\n",
    "train_data['Questions']=train_data['Questions'].apply(lambda x:' '.join([''.join([char for char in w if char not in punctuations]) for w in x.split()]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vw5EEzCpYLQl",
    "outputId": "348a5e50-471f-40fb-c8fa-b61bd3f28b18"
   },
   "outputs": [],
   "source": [
    "#dropping the rows with empty values of Question after filtering\n",
    "\n",
    "for j in range(len(train_data['Questions'])):\n",
    "  if len(train_data['Questions'][j])==0:\n",
    "     train_data.drop(j,inplace=True)\n",
    "train_data=train_data.reset_index(drop=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z3vqRKOKbk_N"
   },
   "outputs": [],
   "source": [
    "#using lemmatization on the questions of the train and test dataset\n",
    "def lemmatization(text):\n",
    "    pos_dict = {\n",
    "        'N': 'n',  # Noun\n",
    "        'V': 'v',  # Verb\n",
    "        'R': 'r',  # Adverb\n",
    "        'J': 'a'   # Adjective\n",
    "    }\n",
    "    pos_tags = pos_tag(text)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemma=[]\n",
    "    for word, tag in pos_tags:\n",
    "        if (tag[0].upper() not in pos_dict.keys()):\n",
    "          pos='n'\n",
    "        else:\n",
    "          pos= pos_dict[tag[0].upper()]\n",
    "        lemma.append(lemmatizer.lemmatize(word,pos=pos))\n",
    "    return lemma\n",
    "\n",
    "train_data['Questions']=train_data['Questions'].apply(lambda x : lemmatization(x.split()))\n",
    "train_data[\"Questions\"]=train_data[\"Questions\"].apply(lambda x : \" \".join(x))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pL0HGU975TWq",
    "outputId": "3fbbb00c-b275-4554-f055-8a9b96d6bffa"
   },
   "outputs": [],
   "source": [
    "#Analysing certain parameters about the train data\n",
    "\n",
    "def tokenize_question(text):\n",
    "    return text.split()\n",
    "\n",
    "questions = train_data['Questions'].tolist()\n",
    "\n",
    "print('The total number of words in the data is: ', sum([len(text.split()) for text in questions]))\n",
    "\n",
    "\n",
    "\n",
    "question_vect = CountVectorizer(tokenizer=tokenize_question)\n",
    "questions=question_vect.fit_transform(questions)\n",
    "\n",
    "print('The number of words in the vocabulary is: ', len(question_vect.vocabulary_))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the output after preprocessing\n",
    "\n",
    "# train_data.to_csv(\"Data_570000_preprocessed.csv\",index=False)\n",
    "train_data.to_csv(\"Data_preprocessed.csv\",index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=pd.read_csv(\"Data_570000_preprocessed.csv\") #training dataset\n",
    "test_data=pd.read_csv(\"Data_preprocessed.csv\") #testing dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3vvD1AOZOvMk",
    "outputId": "8a1b58c4-e7f1-4864-c137-a328a8141f11"
   },
   "outputs": [],
   "source": [
    "#exploration about the tags column and selecting the top tags \n",
    "\n",
    "\n",
    "#define the top tags count\n",
    "tags_top=5\n",
    "\n",
    "tags = train_data['Tags'].tolist()\n",
    "tags_Freq=Counter(tags)\n",
    "\n",
    "print(\"Total number of unique tags : \",len(tags_Freq.keys()))\n",
    "tags2=zip(tags_Freq.keys(),tags_Freq.values())\n",
    "\n",
    "tags2=sorted(tags2,key=lambda x:x[1],reverse=True)\n",
    "total_frequency=sum(tags_Freq.values())\n",
    "\n",
    "\n",
    "\n",
    "top_tags=[tags2[j][0] for j in range(tags_top)]\n",
    "top_tags_values=[tags_Freq[tags2[j][0]] for j in range(tags_top)]\n",
    "\n",
    "\n",
    "# plt.bar(top_300_tags, top_300_tags_values, width=0.5, color='r')\n",
    "# plt.xlabel('Tags')\n",
    "# plt.ylabel('Frequencies')\n",
    "# plt.title('Top 20 Tags and Frequencies')\n",
    "# plt.xticks(rotation=90)  # Rotate the x-axis labels for better visibility\n",
    "# plt.tight_layout()  # Adjust the layout to prevent label cutoff\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_each=tags_Freq[top_300_tags[-1]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe=pd.DataFrame()\n",
    "n_each=tags_Freq[top_300_tags[-1]]\n",
    "for tag in top_300_tags:\n",
    "    current_df=train_data[train_data[\"Tags\"]==tag]\n",
    "    sampled_df = current_df.sample(n=n_each, random_state=42)\n",
    "    dataframe=dataframe.append(sampled_df, ignore_index=True)\n",
    "train_data=shuffle(dataframe).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FJdoFeOnaMV9"
   },
   "outputs": [],
   "source": [
    "#performing the encoding of the tags column\n",
    "\n",
    "def filter_data_by_most_common_tags(data, common_tags):\n",
    "    filtered_data = data[data[\"Tags\"].isin(common_tags)]\n",
    "    return filtered_data\n",
    "\n",
    "\n",
    "def one_hot(column,data): #count vectorizer feature set\n",
    "  # Perform one-hot encoding using get_dummies()\n",
    "  one_hot_encoded = pd.get_dummies(data[column],prefix=\"tag\")\n",
    "\n",
    "  # Concatenate the one-hot encoded columns with the original dataframe\n",
    "  data_extended = pd.concat([data, one_hot_encoded], axis=1)\n",
    "  data_extended.drop(['Tags'],inplace=True,axis=1)\n",
    "\n",
    "  return data_extended\n",
    "\n",
    "def label_encoding(data, most_common_tags):#tfidf vectorizer\n",
    "    v = {}\n",
    "    for j in range(len(most_common_tags)):\n",
    "        v[most_common_tags[j]] = j\n",
    "    data[\"Tags\"] = data[\"Tags\"].apply(lambda x: v[x] if x in v else -1)\n",
    "    return data\n",
    "\n",
    "\n",
    "Final_train=filter_data_by_most_common_tags(train_data,top_tags)\n",
    "\n",
    "Final_test=filter_data_by_most_common_tags(test_data,top_tags)\n",
    "#performing the one hot encoding of the data \n",
    "\n",
    "\n",
    "Final_train=one_hot(\"Tags\",Final_train)\n",
    "\n",
    "Final_train=Final_train.reset_index(drop=True)\n",
    "\n",
    "Final_test=one_hot(\"Tags\",Final_test)\n",
    "\n",
    "Final_test=Final_test.reset_index(drop=True)\n",
    "\n",
    "#Another Approach (tfidf vectorizer)\n",
    "# Final_train=label_encoding(Final_train,most_common_tags)\n",
    "# Final_test=label_encoding(Final_test,most_common_tags)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizing the questions and storing it in Text_Tokenized column \n",
    "\n",
    "train_data['Text_Tokenized'] = train_data['Questions'].str.lower().apply(word_tokenize)\n",
    "\n",
    "\n",
    "Final_train['Text_Tokenized'] = Final_train['Questions'].str.lower().apply(word_tokenize)\n",
    "\n",
    "test_data['Text_Tokenized'] = test_data['Questions'].str.lower().apply(word_tokenize)\n",
    "\n",
    "\n",
    "Final_test['Text_Tokenized'] = Final_test['Questions'].str.lower().apply(word_tokenize)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training the word2vec model\n",
    "\n",
    "vector_size_n_w2v = 100\n",
    "\n",
    "w2v_model = Word2Vec(train_data['Text_Tokenized'],\n",
    "                     vector_size=vector_size_n_w2v,\n",
    "                     window=3,\n",
    "                     min_count=1,\n",
    "                     sg=0, # 0=CBOW, 1=Skip-gram\n",
    "                     epochs=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the word2vec model\n",
    "\n",
    "# w2v_model.save(\"word2vec_model\")\n",
    "\n",
    "# pk.dump(vector_size_n_w2v, open('vector_size_w2v_metric.pkl', 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#check which words are similar to another given word\n",
    "# w2v_model.wv.most_similar(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using pretrained google model \n",
    "\n",
    "w2v_model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary = True) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting the data into vectors and taking element wise average to describe the sentence \n",
    "text_vect_avg = []\n",
    "# dim=w2v_model['computer'].shape[0]\n",
    "for idx,text in enumerate(Final_train[\"Text_Tokenized\"]):\n",
    "    current=np.array([w2v_model.wv[i] for i in text if i in w2v_model.wv])\n",
    "    if (idx%100==0): print(idx)\n",
    "    if current.size:\n",
    "        text_vect_avg.append(current.mean(axis=0))\n",
    "    else:\n",
    "        text_vect_avg.append(np.zeros(vector_size_n_w2v, dtype=float)) # the same vector size must be used here as for model training\n",
    "        \n",
    "\n",
    "    \n",
    "        \n",
    "df_Machine_Learning = pd.DataFrame(text_vect_avg)\n",
    "\n",
    "df_Machine_Learning.columns = ['Element_' + str(i+1) for i in range(0, df_Machine_Learning.shape[1])] #Naming the columns of the word2vec dataset\n",
    "\n",
    "\n",
    "Final_train= pd.concat([df_Machine_Learning,Final_train.iloc[:,1:-1]], axis=1, sort=False) # concatenating the tags and the word2vec dataset\n",
    "\n",
    "model_type=\"Word2vec\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting the data into vectors and taking element wise average to describe the sentence \n",
    "text_vect_avg = []\n",
    "# dim=w2v_model['computer'].shape[0]\n",
    "for idx,text in enumerate(Final_test[\"Text_Tokenized\"]):\n",
    "    current=np.array([w2v_model.wv[i] for i in text if i in w2v_model.wv])\n",
    "    if (idx%100==0): print(idx)\n",
    "    if current.size:\n",
    "        text_vect_avg.append(current.mean(axis=0))\n",
    "    else:\n",
    "        text_vect_avg.append(np.zeros(vector_size_n_w2v, dtype=float)) # the same vector size must be used here as for model training\n",
    "        \n",
    "\n",
    "    \n",
    "        \n",
    "df_Machine_Learning = pd.DataFrame(text_vect_avg)\n",
    "\n",
    "df_Machine_Learning.columns = ['Element_' + str(i+1) for i in range(0, df_Machine_Learning.shape[1])] #Naming the columns of the word2vec dataset\n",
    "\n",
    "\n",
    "Final_test= pd.concat([df_Machine_Learning,Final_test.iloc[:,1:-1]], axis=1, sort=False) # concatenating the tags and the word2vec dataset\n",
    "\n",
    "model_type=\"Word2vec\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the saved word2vec model\n",
    "\n",
    "w2v_model_reloaded = Word2Vec.load(\"word2vec/word2vec_model\")\n",
    "vector_size_n_reloaded = pk.load(open(\"word2vec/vector_size_w2v_metric.pkl\",'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KKLqkfsa-bVI"
   },
   "outputs": [],
   "source": [
    "#tdidf approach\n",
    "\n",
    "def tokenize_question(text):\n",
    "    return text.split()\n",
    "\n",
    "def filter_number_features(name):\n",
    "  if name[0] in '0123456789' or len(name)<=3:\n",
    "    return False\n",
    "  return True\n",
    "tfidf_vect = TfidfVectorizer(tokenizer=tokenize_question,\n",
    "                               stop_words='english',\n",
    "                               min_df=4,\n",
    "                               max_df=0.5,max_features=1000)\n",
    "\n",
    "X_train_tfidf = tfidf_vect.fit_transform(Final_train[\"Questions\"]).todense()\n",
    "# print('The number of words in the vocabulary is: ', len(tfidf_vect.vocabulary_))\n",
    "\n",
    "\n",
    "\n",
    "#get the feature names\n",
    "feature_names=tfidf_vect.get_feature_names_out()\n",
    "\n",
    "# Get the IDF scores\n",
    "idf_scores = tfidf_vect.idf_\n",
    "\n",
    "Final_Feature_Set=[]\n",
    "for idx,feature_name in enumerate(feature_names):\n",
    "    if filter_number_features(feature_name):\n",
    "       Final_Feature_Set.append([feature_name,idf_scores[idx]])\n",
    "\n",
    "Final_Feature_Set=sorted(Final_Feature_Set,key=lambda x :x[1],reverse=True)\n",
    "Final_Feature_Set=[x[0] for x in Final_Feature_Set]\n",
    "\n",
    "\n",
    "\n",
    "df_train = pd.DataFrame(X_train_tfidf, columns=tfidf_vect.get_feature_names_out())\n",
    "\n",
    "df_train=df_train[Final_Feature_Set]\n",
    "\n",
    "Final_train= pd.concat([df_train,Final_train.iloc[:,1:]], axis=1, sort=False) # concatenating the tags and the tdidf dataset\n",
    "\n",
    "model_type=\"Tdidf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For embedded layer in keras\n",
    "\n",
    "\n",
    "# Tokenize the text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(Final_train[\"Questions\"])\n",
    "word_index = tokenizer.word_index\n",
    "sequences = tokenizer.texts_to_sequences(Final_train[\"Questions\"])\n",
    "\n",
    "# Pad sequences\n",
    "max_sequence_length = max(len(seq) for seq in sequences)\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fEhjmrlXvQAh"
   },
   "outputs": [],
   "source": [
    "X_train=Final_train.iloc[:,:-1*len(top_tags)]\n",
    "\n",
    "Y_train=Final_train.iloc[:,-1*len(top_tags):]\n",
    "\n",
    "# X_train=Final_train.iloc[:,:-1]\n",
    "\n",
    "# Y_train=Final_train.iloc[:,-1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HDT62ACPaSk-"
   },
   "outputs": [],
   "source": [
    "# X_train, X_test, Y_train, Y_test = train_test_split(padded_sequences, Y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_train,Y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "# Y_train=np.array([np.argmax(Y_train.iloc[idx,:]) for idx in range(Y_train.shape[0])])\n",
    "# Y_test=np.array([np.argmax(Y_test.iloc[idx,:]) for idx in range(Y_test.shape[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x=Final_test.iloc[:,:-1*len(top_tags)]\n",
    "test_y=Final_test.iloc[:,-1*len(top_tags):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using oversampling to deal with class imbalance \n",
    "\n",
    "\n",
    "# Print the class distribution before oversampling\n",
    "print(\"Class distribution before oversampling:\")\n",
    "unique, counts = np.unique(Y_train, return_counts=True)\n",
    "for label, count in zip(unique, counts):\n",
    "    print(f\"Class {label}: {count} samples\")\n",
    "\n",
    "Apply Random Oversampling\n",
    "oversampler = RandomOverSampler(random_state=42)\n",
    "X_train_resampled, Y_train_resampled = oversampler.fit_resample(X_train, Y_train)\n",
    "\n",
    "# Print the class distribution after oversampling\n",
    "print(\"\\nClass distribution after oversampling:\")\n",
    "unique, counts = np.unique(Y_train_resampled, return_counts=True)\n",
    "for label, count in zip(unique, counts):\n",
    "    print(f\"Class {label}: {count} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Storing the results for models \n",
    "Results={}\n",
    "\n",
    "vectorization=[\"Tdidf\",\"Word2vec\"]\n",
    "\n",
    "Models_tdidf=[\"Logistic\",\"Svm\",\"Naivebayes\",\"Ann\",\"Gradientboosting\"]\n",
    "Models_word2vec=[\"Logistic\",\"Svm\",\"Rnn\",\"Gradientboosting\"]\n",
    "Tags=['5','20','100']\n",
    "\n",
    "for method in vectorization:\n",
    "    Results[method]={}\n",
    "    if method==\"Tdidf\":\n",
    "        for model in Models_tdidf:\n",
    "            Results[method][model]={}\n",
    "            for Tag in Tags:\n",
    "                Results[method][model][Tag]=-1\n",
    "                \n",
    "    else:\n",
    "        for model in Models_word2vec:\n",
    "            Results[method][model]={}\n",
    "            for Tag in Tags:\n",
    "                Results[method][model][Tag]=-1\n",
    "        \n",
    "       \n",
    "            \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to calculate the evaluation metrics\n",
    "\n",
    "def eval_metrics(y_test, y_predicted, model,test,print_metrics=True):\n",
    "    global Results,model_type,top_300_tags\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_predicted)\n",
    "    precision = precision_score(y_test, y_predicted, average='weighted')\n",
    "    recall = recall_score(y_test, y_predicted, average='weighted')\n",
    "    f1 = f1_score(y_test, y_predicted, average='weighted')\n",
    "    if test:\n",
    "         Results[model_type][model][str(len(top_300_tags))]=round(accuracy*100,2)\n",
    "\n",
    "    if print_metrics:\n",
    "        print(\"f1: %.3f - precision: %.3f - recall: %.3f - accuracy: %.3f\" % (\n",
    "            f1, precision, recall, accuracy))\n",
    "    return f1, precision, recall, accuracy\n",
    "\n",
    "def print_score(y_pred,y_test):\n",
    "  print('Jacard score: {}'.format(j_score(y_test, y_pred)))\n",
    "  print('----')\n",
    "    \n",
    "def convert(pred,original,flag):\n",
    "    if flag==\"tdidf\":\n",
    "        original=[np.argmax(original.iloc[idx,:]) for idx in range(original.shape[0])]\n",
    "    else:\n",
    "        original=[np.argmax(original[idx,:]) for idx in range(original.shape[0])]\n",
    "\n",
    "    pred=[np.argmax(pred[idx,:]) for idx in range(pred.shape[0])]\n",
    "    return original,pred    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Results.json', 'w') as f:\n",
    "    json.dump(Results, f)     \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('Results.json')\n",
    "Results=json.load(f) #Loading json file as python dictionary\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results['Tdidf']['Logistic']['5']=81.48\n",
    "# Results['Tdidf']['Logistic']['20']=44.65\n",
    "# Results['Tdidf']['Logistic']['100']=29.31\n",
    "# Results['Tdidf']['Svm']['5']=84.42\n",
    "# Results['Tdidf']['Svm']['20']=48.61\n",
    "# Results['Tdidf']['Svm']['100']=31.52\n",
    "# Results['Tdidf']['Naivebayes']['5']=72.97\n",
    "# Results['Tdidf']['Naivebayes']['20']=18.98\n",
    "# Results['Tdidf']['Naivebayes']['100']=6.39\n",
    "# Results['Word2vec']['Logistic']['5']=79.47\n",
    "# Results['Word2vec']['Logistic']['20']=35.13\n",
    "# Results['Word2vec']['Logistic']['100']=17.29\n",
    "# Results['Word2vec']['Svm']['5']=90.23\n",
    "# Results['Word2vec']['Svm']['20']=42.81\n",
    "# Results['Word2vec']['Svm']['100']=20.48\n",
    "# Results['Word2vec']['Rnn']['5']=88.43\n",
    "# Results['Word2vec']['Rnn']['20']=51.58\n",
    "# Results['Word2vec']['Rnn']['100']=21.65\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8aJNW7iiwCQa"
   },
   "source": [
    "**Machine learning using classification algos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 117
    },
    "id": "npYTf3n9xBFB",
    "outputId": "2d5e3247-87ee-4143-b7a9-7beaf55f692d"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Classifier - Algorithm - Logistic Regression (gridCv)\n",
    "\n",
    "log_clf = OneVsRestClassifier(LogisticRegression())\n",
    "param_grid = {\n",
    "    'estimator__C': [0.1, 1, 10],  # Regularization parameter C\n",
    "    'estimator__solver': ['liblinear', 'lbfgs']  # Solver algorithm\n",
    "}\n",
    "\n",
    "# grid search\n",
    "grid_search = GridSearchCV(log_clf, param_grid, cv=5)\n",
    "grid_search.fit(X_train, Y_train)\n",
    "\n",
    "# The best parameters\n",
    "best_params = grid_search.best_params_\n",
    "print(best_params)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifier - Algorithm - Logistic Regression \n",
    "\n",
    "log_clf = OneVsRestClassifier(LogisticRegression(C=10, solver='liblinear',verbose=True))\n",
    "\n",
    "log_clf.fit(X_train, Y_train)\n",
    "\n",
    "Y_train_predict = log_clf.predict(X_train)\n",
    "\n",
    "\n",
    "\n",
    "Y_test_predict=log_clf.predict(X_test)\n",
    "\n",
    "print(\"LogisticR train Score :\")\n",
    "\n",
    "eval_metrics(Y_train,Y_train_predict,\"Logistic\",0)\n",
    "\n",
    "print(\"LogisticR test Score : \")\n",
    "\n",
    "eval_metrics(Y_test,Y_test_predict,\"Logistic\",1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifier - Algorithm - Gradient Boosting (gridCv)\n",
    "\n",
    "\n",
    "#the parameter grid\n",
    "param_grid = {\n",
    "    'estimator__n_estimators': [100],\n",
    "    'estimator__learning_rate': [0.1, 0.01, 0.001],\n",
    "    'estimator__max_depth': [3, 5, 7],\n",
    "    'estimator__subsample': [0.8, 1.0],\n",
    "    \n",
    "}\n",
    "\n",
    "#  Gradient Boosting classifier\n",
    "GB = OneVsRestClassifier(XGBClassifier())\n",
    "\n",
    "\n",
    "# Grid search\n",
    "grid_search = GridSearchCV(GB, param_grid, cv=5,verbose=2)\n",
    "grid_search.fit(X_train, Y_train)\n",
    "\n",
    "# the best parameters\n",
    "best_params = grid_search.best_params_\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifier - Algorithm - Gradient Boosting \n",
    "\n",
    "GB = OneVsRestClassifier(XGBClassifier(learning_rate= 0.1,max_depth=7,n_estimators=300,subsample=0.8))\n",
    "\n",
    "GB.fit(X_train, Y_train)\n",
    "\n",
    "Y_train_predict = GB.predict(X_train)\n",
    "\n",
    "\n",
    "\n",
    "Y_test_predict=GB.predict(X_test)\n",
    "\n",
    "print(\"GradientB train Score :\")\n",
    "\n",
    "eval_metrics(Y_train,Y_train_predict,\"GradientBoosting\",0)\n",
    "\n",
    "print(\"GradientB test Score : \")\n",
    "\n",
    "eval_metrics(Y_test,Y_test_predict,\"Gradientboosting\",1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C8TB-KNa7Wzu"
   },
   "outputs": [],
   "source": [
    "# Classifier - Algorithm - SVM (grid_cv)\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'estimator__C': [0.1, 1, 10],\n",
    "    'estimator__kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "    'estimator__degree': [2, 3, 4],\n",
    "    'estimator__gamma': ['scale', 'auto']\n",
    "}\n",
    "\n",
    "SVM = OneVsRestClassifier(svm.SVC())\n",
    "\n",
    "# grid search\n",
    "grid_search = GridSearchCV(SVM, param_grid, cv=5)\n",
    "grid_search.fit(X_train, Y_train)  \n",
    "\n",
    "#the best parameters\n",
    "best_params = grid_search.best_params_\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifier - Algorithm - SVM \n",
    "\n",
    "\n",
    "# Train SVM model\n",
    "SVM = OneVsRestClassifier(svm.SVC(C=10, degree=2, gamma='scale', kernel='rbf',verbose=True))\n",
    "SVM.fit(X_train,Y_train)\n",
    "\n",
    "\n",
    "\n",
    "#  predict the labels \n",
    "predictions_SVM_train,original_train = convert(SVM.predict(X_train),Y_train,\"tdidf\")\n",
    "predictions_SVM_test,original_test = convert(SVM.predict(test_x),test_y,\"tdidf\")\n",
    "\n",
    "# Use accuracy_score function to get the accuracy\n",
    "\n",
    "print(\"SVM train Score :\")\n",
    "\n",
    "eval_metrics(original_train,predictions_SVM_train,\"Svm\",0)\n",
    "\n",
    "print(\"SVM test Accuracy Score : \")\n",
    "\n",
    "eval_metrics(original_test,predictions_SVM_test,\"Svm\",1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RvS0elEtfNdt",
    "outputId": "6c2cf7a5-b657-4f4b-a779-15b16e99e929",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Classifier - Algorithm - Naive Bayes (gridCv)\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'estimator__alpha': [0.1, 0.5, 1.0],  # Smoothing parameter\n",
    "    'estimator__fit_prior': [True, False]  # Whether to learn class prior probabilities\n",
    "}\n",
    "\n",
    "# Create the Naive Bayes classifier\n",
    "naive = OneVsRestClassifier(naive_bayes.MultinomialNB())\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(naive, param_grid, cv=5)\n",
    "grid_search.fit(X_train, Y_train)\n",
    "\n",
    "# Print the best parameters\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifier - Algorithm - Naive Bayes \n",
    "\n",
    "\n",
    "Naive = OneVsRestClassifier(naive_bayes.MultinomialNB(alpha=0.5,fit_prior=True))\n",
    "\n",
    "Naive.fit(X_train,Y_train)\n",
    "# predict the labels on validation dataset\n",
    "\n",
    "predictions_NB_test = Naive.predict(X_test)\n",
    "predictions_NB_train = Naive.predict(X_train)\n",
    "# Use accuracy_score function to get the accuracy\n",
    "# Use accuracy_score function to get the accuracy\n",
    "\n",
    "print(\"Naivebayes train Score :\")\n",
    "\n",
    "eval_metrics(Y_train,predictions_NB_train,\"Naivebayes\",0)\n",
    "\n",
    "print(\"Naivebayes test Score : \")\n",
    "\n",
    "eval_metrics(Y_test,predictions_NB_test,\"Naivebayes\",1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Neural Network part(ANN)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding the best parameter set for Rnn model\n",
    "\n",
    "def build_model(units=64, dropout_rate=0.2,reg_lambda=0.001, activation1='relu',activation2='relu',optimizer='adam'):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units, activation=activation1, input_shape=(X_train.shape[1],),kernel_regularizer=l2(reg_lambda)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(Y_train.shape[1], activation=activation2))\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "# KerasClassifier wrapper\n",
    "model = KerasClassifier(build_fn=build_model)\n",
    "\n",
    "# parameter grid\n",
    "param_grid = {\n",
    "    'epochs': [20,30], \n",
    "    'batch_size': [32, 64],  \n",
    "    'optimizer': ['rmsprop', 'adam'], \n",
    "    'reg_lambda': [0.001, 0.01, 0.1], \n",
    "    'activation1': ['relu', 'sigmoid'],  \n",
    "    'activation2': ['relu', 'sigmoid'], \n",
    "    'dropout_rate': [0.2, 0.4],  \n",
    "    'units': [32, 64]  \n",
    "}\n",
    "\n",
    "\n",
    "# GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=3,verbose=2)\n",
    "grid_search.fit(X_train, Y_train)\n",
    "\n",
    "\n",
    "# best parameters and best score\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "print(\"Best Parameters: \", best_params)\n",
    "print(\"Best Score: \", best_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params={'activation1': 'relu','activation2': 'sigmoid', 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'optimizer': 'adam', 'reg_lambda': 0.001, 'units': 64}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(best_params['units'], activation=best_params['activation1'], input_shape=(X_train.shape[1],),kernel_regularizer=l2(best_params['reg_lambda'])))\n",
    "model.add(Dropout(best_params['dropout_rate']))\n",
    "model.add(Dense(Y_train.shape[1], activation=best_params['activation2']))\n",
    "model.compile(loss='binary_crossentropy', optimizer=best_params['optimizer'], metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, Y_train, batch_size=best_params['batch_size'], epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training data results\n",
    "\n",
    "Y_pred_train=model.predict(X_train)\n",
    "original_train,pred_train=convert(Y_pred_train,Y_train,\"tdidf\")\n",
    "eval_metrics(pred_train,original_train,\"Ann\",0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test data results\n",
    "\n",
    "Y_pred_test=model.predict(X_test)\n",
    "original_test,pred_test=convert(Y_pred_test,Y_test,\"tdidf\")\n",
    "eval_metrics(pred_test,original_test,\"Ann\",1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Neural Network part (RNN)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert data to numpy arrays\n",
    "X_train = np.array(X_train).astype(np.float32)\n",
    "X_test = np.array(X_test).astype(np.float32)\n",
    "test_x=np.array(test_x).astype(np.float32)\n",
    "# Reshape the data for LSTM input\n",
    "X_train_lstm = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))\n",
    "X_test_lstm = np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1]))\n",
    "\n",
    "x_test_lstm=np.reshape(test_x, (test_x.shape[0], 1, test_x.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 236
    },
    "id": "K_xjTgctJNu1",
    "outputId": "85fa9632-59fb-4eb2-f06b-74a1275a9b0e"
   },
   "outputs": [],
   "source": [
    "#Finding the best parameter set for Rnn model\n",
    "\n",
    "def build_model(units=64, dropout_rate=0.2,reg_lambda=0.001, activation='relu',optimizer='adam'):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units, input_shape=(X_train_lstm.shape[1], X_train_lstm.shape[2]), kernel_regularizer=l2(reg_lambda)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(Y_train.shape[1], activation=activation))\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "# KerasClassifier wrapper\n",
    "model = KerasClassifier(build_fn=build_model)\n",
    "\n",
    "# parameter grid\n",
    "param_grid = {\n",
    "    'epochs': [50,100],\n",
    "    'batch_size':[64],\n",
    "    'optimizer': ['rmsprop'],\n",
    "    'reg_lambda':[0.001],\n",
    "    'activation': ['relu'],\n",
    "    'dropout_rate': [0.2],\n",
    "    'units': [64]\n",
    "}\n",
    "\n",
    "\n",
    "# GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=3)\n",
    "grid_search.fit(X_train_lstm, Y_train)\n",
    "\n",
    "\n",
    "# best parameters and best score\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "print(\"Best Parameters: \", best_params)\n",
    "print(\"Best Score: \", best_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sx7FMU046r4z"
   },
   "outputs": [],
   "source": [
    "# best_params = grid_search.best_params_\n",
    "# best_score = grid_search.best_score_\n",
    "\n",
    "# print(\"Best Parameters: \", best_params)\n",
    "# print(\"Best Score: \", best_score)\n",
    "\n",
    "best_params={'activation': 'relu', 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'optimizer': 'rmsprop', 'reg_lambda': 0.001, 'units': 64}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training the model based on the best parameters\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(best_params['units'], input_shape=(X_train_lstm.shape[1], X_train_lstm.shape[2]), kernel_regularizer=l2(best_params['reg_lambda'])))\n",
    "model.add(Dropout(best_params['dropout_rate']))\n",
    "model.add(Dense(Y_train.shape[1], activation=best_params['activation']))\n",
    "model.compile(loss='binary_crossentropy', optimizer=best_params['optimizer'], metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit(X_train_lstm, Y_train, batch_size=best_params['batch_size'], epochs=best_params['epochs'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**using inbuilt embedding layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "embedding_dim = 100\n",
    "hidden_dim = 64\n",
    "num_classes = len(label_names)\n",
    "batch_size = 64\n",
    "num_epochs = 5\n",
    "\n",
    "# Create the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1, embedding_dim, input_length=max_sequence_length))\n",
    "model.add(LSTM(hidden_dim,kernel_regularizer=l2(0.001)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, Y_train, batch_size=batch_size, epochs=num_epochs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training data results\n",
    "\n",
    "Y_pred_train=model.predict(X_train_lstm)\n",
    "original_train,pred_train=convert(Y_pred_train,Y_train,\"tdidf\")\n",
    "eval_metrics(pred_train,original_train,\"Rnn\",0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test data results\n",
    "\n",
    "Y_pred_test=model.predict(x_test_lstm)\n",
    "original_test,pred_test=convert(Y_pred_test,test_y,\"tdidf\")\n",
    "eval_metrics(pred_test,original_test,\"Rnn\",1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tdidf=[]\n",
    "Tags_5_tdidf=[]\n",
    "Tags_20_tdidf=[]\n",
    "Tags_100_tdidf=[]\n",
    "\n",
    "x_word2vec=[]\n",
    "Tags_5_word2vec=[]\n",
    "Tags_20_word2vec=[]\n",
    "Tags_100_word2vec=[]\n",
    "\n",
    "for vectorization in Results.keys():\n",
    "    for model in Results[vectorization].keys():\n",
    "        if vectorization=='Tdidf':\n",
    "             x_tdidf.append(model)\n",
    "        else:\n",
    "             x_word2vec.append(model)\n",
    "        for Tags in Results[vectorization][model]:\n",
    "            if Tags=='5' and vectorization=='Tdidf':\n",
    "                Tags_5_tdidf.append(Results['Tdidf'][model][Tags])\n",
    "                continue\n",
    "            if Tags=='20' and vectorization=='Tdidf':\n",
    "                Tags_20_tdidf.append(Results['Tdidf'][model][Tags])    \n",
    "                continue\n",
    "            if Tags=='100' and vectorization=='Tdidf':\n",
    "                Tags_100_tdidf.append(Results['Tdidf'][model][Tags])\n",
    "                continue\n",
    "            if model!='Naivebayes':    \n",
    "                if Tags=='5':\n",
    "                    Tags_5_word2vec.append(Results[vectorization][model][Tags])\n",
    "                if Tags=='20':\n",
    "                    Tags_20_word2vec.append(Results[vectorization][model][Tags])    \n",
    "                if Tags=='100':\n",
    "                    Tags_100_word2vec.append(Results[vectorization][model][Tags])\n",
    "\n",
    "                    #dealing with svm 100 as not calculated yet                    \n",
    "\n",
    "\n",
    "vectorization=[]\n",
    "vectorization.append(x_tdidf)\n",
    "vectorization.append(x_word2vec)\n",
    "\n",
    "Tags_Y=[[Tags_5_tdidf,Tags_20_tdidf,Tags_100_tdidf],[Tags_5_word2vec,Tags_20_word2vec,Tags_100_word2vec]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "bar_width = 0.2\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Set the colors for the bars\n",
    "color1 = '#FF5A5F'  # Red color for 5\n",
    "color2 = '#00A699'  # Green color for 20\n",
    "color3 = 'black'    # Black color for 100\n",
    "\n",
    "for idx, x_labels in enumerate(vectorization):\n",
    "    x = np.arange(len(x_labels))\n",
    "    if idx == 0:\n",
    "        title = \"Tdidf\"\n",
    "    else:\n",
    "        title = \"Word2vec\"\n",
    "\n",
    "    # Plot the bars\n",
    "    bar1 = ax[idx].bar(x - bar_width, Tags_Y[idx][0], width=bar_width, color=color1, label='5')\n",
    "    bar2 = ax[idx].bar(x, Tags_Y[idx][1], width=bar_width, color=color2, label='20')\n",
    "    bar3 = ax[idx].bar(x + bar_width, Tags_Y[idx][2], width=bar_width, color=color3, label='100')\n",
    "\n",
    "    # Set the x-axis tick labels\n",
    "    ax[idx].set_xticks(x)\n",
    "    ax[idx].set_xticklabels(x_labels)\n",
    "\n",
    "    # Set the axis labels and title\n",
    "    ax[idx].set_xlabel(\"Model Name\", fontsize=10)\n",
    "    ax[idx].set_ylabel(\"Test Accuracy (%)\", fontsize=10)\n",
    "    ax[idx].set_title(title, fontsize=14,y=1.03)\n",
    "    ax[idx].xaxis.set_label_coords(0.5, -0.1)\n",
    "\n",
    "    # Set the legend with a heading and increased font size\n",
    "    ax[idx].legend(title='Top Tags', frameon=False, fontsize=10, bbox_to_anchor=(1.15, 1), loc='upper right')\n",
    "\n",
    "    # Remove spines and ticks on the top and right sides\n",
    "    ax[idx].spines['top'].set_visible(False)\n",
    "    ax[idx].spines['right'].set_visible(False)\n",
    "    ax[idx].tick_params(right=False, top=False)\n",
    "\n",
    "    # Add grid lines\n",
    "    ax[idx].grid(axis='y', linestyle='--', alpha=0.5)\n",
    "\n",
    "    # Add tooltip at the top of each bar\n",
    "    for rect in bar1 + bar2 + bar3:\n",
    "        height = rect.get_height()\n",
    "        ax[idx].annotate(f'{height}', xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                         xytext=(6, 4), textcoords=\"offset points\",\n",
    "                         ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "    # Adjust the bottom margin to add space between xlabel and xticks\n",
    "    ax[idx].tick_params(axis='x', which='major', pad=4)\n",
    "\n",
    "# Adjust the layout and padding\n",
    "fig.tight_layout(pad=-1)\n",
    "plt.subplots_adjust(left=0.1,\n",
    "                    bottom=0.1,\n",
    "                    right=0.9,\n",
    "                    top=0.9,\n",
    "                    wspace=0.3,\n",
    "                    hspace=0.4)\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Web Scraping Part to fetch new set of questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scraping new set of questions\n",
    "import pandas as pd\n",
    "Questions=[] #array to store the questions\n",
    "Questions=pd.read_csv(\"New_data.csv\")\n",
    "\n",
    "\n",
    "Questions=list(zip(Questions['Questions'],Questions[\"Tags\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(Questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "start=time.time()\n",
    "for pageNumber in range(6000,10000):\n",
    "     \n",
    "    response=requests.get(\"https://stackoverflow.com/questions\",params={\"tab\":\"newest\",\"page\":pageNumber,\"pagesize\":50,'sort':\"MostVotes\"})\n",
    "\n",
    "    data=BeautifulSoup(response.text,'html.parser' ) #parsing the html text\n",
    "\n",
    "    req_data=data.find(id=\"questions\")\n",
    "\n",
    "    new_data=req_data.find_all(\"h3\", class_=\"s-post-summary--content-title\") # the tag that contains the question info\n",
    "\n",
    "    print(time.time()-start,len(Questions),pageNumber) #checking the current stats of operation\n",
    "\n",
    "    for element in new_data:\n",
    "        link=element.a.attrs['href']\n",
    "\n",
    "        response=requests.get(f\"https://stackoverflow.com/{link}\")  #fetching the content related to the question\n",
    "        time.sleep(0.8)\n",
    "\n",
    "        data_questionwise=BeautifulSoup(response.text,'html.parser')\n",
    "        try:\n",
    "\n",
    "\n",
    "            question_wise_title=data_questionwise.find(\"div\",id=\"question-header\").h1.a.string #title\n",
    "\n",
    "            question_wise_desc=data_questionwise.find(\"div\",class_=\"s-prose js-post-body\") #description\n",
    "        \n",
    "            question_wise_score=int(data_questionwise.find(\"div\",class_=\"js-vote-count flex--item d-flex fd-column ai-center fc-theme-body-font fw-bold fs-subheading py4\").string.replace(\" \", \"\")) #score \n",
    "        \n",
    "        except:\n",
    "             continue\n",
    "        \n",
    "        if(question_wise_score<5):\n",
    "            break\n",
    "        \n",
    "        all_paragraphs=question_wise_desc.find_all(\"p\")\n",
    "\n",
    "        total_description_question_wise=\"\"\n",
    "\n",
    "        for para in all_paragraphs:\n",
    "            total_description_question_wise+=para.text\n",
    "\n",
    "        Final_content=question_wise_title+\"\"+total_description_question_wise  #concatenating the title and description\n",
    "\n",
    "        tag_question_wise=data_questionwise.find(\"ul\",class_=\"ml0 list-ls-none js-post-tag-list-wrapper d-inline\").li.text #tag\n",
    "\n",
    "        Questions.append([Final_content,tag_question_wise])\n",
    "    \n",
    "\n",
    "\n",
    "print(len(Questions))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#479\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(Questions,columns=[\"Questions\",\"Tags\"])\n",
    "df.to_csv(\"New_data.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
