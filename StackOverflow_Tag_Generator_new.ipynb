{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ad451/Stackoverflow_tag_generator/blob/main/StackOverflow_Tag_Generator_new.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLtFomOOTrCE"
      },
      "source": [
        "\n",
        "**Importing the required modules**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fy7GyiItTYJx",
        "outputId": "94541196-7d36-42b8-f402-dc33960c4a86"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import requests\n",
        "import random\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "import warnings; warnings.simplefilter('ignore')\n",
        "import re\n",
        "import string\n",
        "from collections import Counter\n",
        "from google.colab import files\n",
        "\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import ToktokTokenizer\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk import pos_tag\n",
        "\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils.class_weight import compute_sample_weight\n",
        "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score,jaccard_score\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn import model_selection, naive_bayes, svm\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense,Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
        "\n",
        "\n",
        "nltk.download('averaged_perceptron_tagger') # required for parts of speech\n",
        "nltk.download('wordnet') # required for parts of speech\n",
        "nltk.download('stopwords') #download the stopwords\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAaTn6ZCW3CB"
      },
      "source": [
        "**Web Scraping the current set of questions for testing the model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NKcc1fyLhTBt"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "Questions=[] #array to store the questions\n",
        "\n",
        "for pageNumber in range(1,401):\n",
        "    response=requests.get(\"https://stackoverflow.com/questions\",params={\"tab\":\"newest\",\"page\":pageNumber,\"pagesize\":50})\n",
        "\n",
        "    data=BeautifulSoup(response.text,'html.parser' ) #parsing the html text\n",
        "\n",
        "    req_data=data.find(id=\"questions\")\n",
        "\n",
        "    new_data=req_data.find_all(\"h3\", class_=\"s-post-summary--content-title\") # the tag that contains the question info\n",
        "\n",
        "\n",
        "\n",
        "    for element in new_data:\n",
        "        link=element.a.attrs['href']\n",
        "\n",
        "        response=requests.get(f\"https://stackoverflow.com/{link}\")  #fetching the content related to the question\n",
        "\n",
        "        data_questionwise=BeautifulSoup(response.text,'html.parser')\n",
        "\n",
        "        question_wise_title=data_questionwise.find(\"div\",id=\"question-header\").h1.a.string #title\n",
        "\n",
        "        question_wise_desc=data_questionwise.find(\"div\",class_=\"s-prose js-post-body\") #description\n",
        "\n",
        "        all_paragraphs=question_wise_desc.find_all(\"p\")\n",
        "\n",
        "        total_description_question_wise=\"\"\n",
        "\n",
        "        for para in all_paragraphs:\n",
        "            total_description_question_wise+=para.text\n",
        "\n",
        "        Final_content=question_wise_title+\"\"+total_description_question_wise  #concatenating the title and description\n",
        "\n",
        "        tag_question_wise=data_questionwise.find(\"ul\",class_=\"ml0 list-ls-none js-post-tag-list-wrapper d-inline\").li.text #tag\n",
        "\n",
        "        Questions.append([Final_content,tag_question_wise])\n",
        "\n",
        "    print(pageNumber)   #checking which page questions have been fetched yet\n",
        "\n",
        "print(len(Questions))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8MnaCTvXb8q"
      },
      "source": [
        "**combine the input questions and tags table**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oXOsnSFrjn9b"
      },
      "outputs": [],
      "source": [
        "###################################### Code for inner combine ######################################\n",
        "df1 = pd.read_csv('Questions.csv', encoding='ISO-8859-1')\n",
        "\n",
        "df2 = pd.read_csv('Tags.csv', encoding='ISO-8859-1')\n",
        "\n",
        "# combined dataframe of questiontags\n",
        "df3 = df1.set_index('Id').join(df2.set_index('Id'))\n",
        "\n",
        "df3=shuffle(df3)\n",
        "\n",
        "df3 = df3.reset_index()\n",
        "\n",
        "\n",
        "\n",
        "###################################### Code for preparing the train_data from the total data ######################################\n",
        "\n",
        "\n",
        "#only taking questions with score greater than or equal to 3\n",
        "\n",
        "df4=df3[df3[\"Score\"]>=3]\n",
        "\n",
        "\n",
        "#generating the list of all unique question ids and also for ranking tags based on their popularity\n",
        "\n",
        "unique_ids=Counter(df4[\"Id\"])\n",
        "\n",
        "q=sorted(zip(Counter(df4[\"Tag\"]).values(),Counter(df7[\"Tag\"]).keys()),reverse=True)\n",
        "\n",
        "rank={}\n",
        "\n",
        "for j in range(len(q)):\n",
        "    rank[q[j][1]]=j+1\n",
        "\n",
        "\n",
        "keys=list(unique_ids.keys())\n",
        "\n",
        "# iterating over each unique question id and assigning only one tag to that based on the ranking of the tag\n",
        "\n",
        "Tags=Counter(df4[\"Tag\"])\n",
        "\n",
        "Final_dataframe={\"Body\":[],\"Title\":[],\"Tags\":[]}\n",
        "\n",
        "for key in keys:\n",
        "\n",
        "    current_df=df4[df4[\"Id\"]==key]\n",
        "    selected_tag=-1\n",
        "    selected_tag_rank=-1\n",
        "    Body=list(current_df[\"Body\"])[0]\n",
        "    Title=list(current_df[\"Title\"])[0]\n",
        "\n",
        "    for tag in current_df[\"Tag\"]:\n",
        "        if rank[tag]>selected_tag_rank:\n",
        "            selected_tag=tag\n",
        "            selected_tag_rank=rank[tag]\n",
        "\n",
        "    Final_dataframe[\"Body\"].append(Body)\n",
        "    Final_dataframe[\"Title\"].append(Title)\n",
        "    Final_dataframe[\"Tags\"].append(selected_tag)\n",
        "\n",
        "df5=pd.DataFrame(Final_dataframe)\n",
        "\n",
        "#concatenating the title and the body columns into the Questions column\n",
        "df5[\"Questions\"]=df5[\"Title\"]+\" \"+df5[\"Body\"]\n",
        "\n",
        "df5.drop([\"Body\",\"Title\"],axis=1,inplace=True)\n",
        "\n",
        "\n",
        "train_data=df5.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kp5z4pMGXlBw"
      },
      "source": [
        "**Machine learning part (Preprocessing and exploration)**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VKY3pgGoXsJg"
      },
      "outputs": [],
      "source": [
        "train_data=pd.read_csv(\"questiontags_train.csv\")\n",
        "test_data=pd.read_csv(\"questiontags_test.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CwaW-h2d4PaE"
      },
      "outputs": [],
      "source": [
        "#rename the columns of the train dataset\n",
        "\n",
        "train_data.drop(train_data.columns[0],axis=1,inplace=True)\n",
        "\n",
        "\n",
        "train_data.rename(columns={\"Title\":\"Questions\",\"Tag\":\"Tags\"},inplace=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#checking for NUll values in the columns\n",
        "\n",
        "train_data.dropna(subset=['Tags'], inplace=True)\n"
      ],
      "metadata": {
        "id": "vnoCwzTWwYM-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ChXcfO2Synd9"
      },
      "outputs": [],
      "source": [
        "#combining the webscraped data and the train_data\n",
        "\n",
        "train_data = pd.concat([train_data, test_data], axis=0, ignore_index=True)\n",
        "train_data=shuffle(train_data)\n",
        "train_data=train_data.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XJmYu5rcynd_"
      },
      "outputs": [],
      "source": [
        "train_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0_RPmQDS7raL"
      },
      "outputs": [],
      "source": [
        "#data cleaning\n",
        "substrings_to_replace = ['</p>', '<p>','\\n','<pre>','</pre>','<a href=\" \">']\n",
        "for substring in substrings_to_replace:\n",
        "    train_data['Questions'] = train_data['Questions'].str.replace(substring, ' ')\n",
        "\n",
        "\n",
        "train_data['Questions']=train_data['Questions'].apply(lambda x : re.sub(r'<code>.*?</code>', ' ', x, flags=re.DOTALL)) #removing any urls\n",
        "\n",
        "train_data['Questions']=train_data['Questions'].apply(lambda x : re.sub('(https?:\\/\\/)?([\\da-z\\.-]+)\\.([a-z\\.]{2,6})([\\/\\w \\.-]*)' , ' ' , x)) #removing any urls\n",
        "train_data['Questions'] = train_data['Questions'].apply(lambda x: ' '.join([w for w in x.split() if len(w) > 3])) #removes small length words (len<3)\n",
        "\n",
        "train_data['Questions']=train_data['Questions'].apply(lambda x : x.lower()) #coverting to lowercase\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G5fomFACHLes"
      },
      "outputs": [],
      "source": [
        "#removing the stop words and the punctuations from test and train dataset\n",
        "\n",
        "punctuations = string.punctuation\n",
        "\n",
        "stop_words=set(nltk.corpus.stopwords.words('english'))\n",
        "train_data['Questions']=train_data['Questions'].apply(lambda x:' '.join([w for w in x.split() if w not in stop_words]))\n",
        "train_data['Questions']=train_data['Questions'].apply(lambda x:' '.join([''.join([char for char in w if char not in punctuations]) for w in x.split()]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vw5EEzCpYLQl"
      },
      "outputs": [],
      "source": [
        "#dropping the rows with empty values of Question after filtering\n",
        "\n",
        "for j in range(len(train_data['Questions'])):\n",
        "  if len(train_data['Questions'][j])==0:\n",
        "     train_data.drop(j,inplace=True)\n",
        "     print(\"yes\")\n",
        "train_data=train_data.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z3vqRKOKbk_N"
      },
      "outputs": [],
      "source": [
        "#using lemmatization on the questions of the train and test dataset\n",
        "def lemmatization(text):\n",
        "    pos_dict = {\n",
        "        'N': 'n',  # Noun\n",
        "        'V': 'v',  # Verb\n",
        "        'R': 'r',  # Adverb\n",
        "        'J': 'a'   # Adjective\n",
        "    }\n",
        "    pos_tags = pos_tag(text)\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemma=[]\n",
        "    for word, tag in pos_tags:\n",
        "        if (tag[0].upper() not in pos_dict.keys()):\n",
        "          pos='n'\n",
        "        else:\n",
        "          pos= pos_dict[tag[0].upper()]\n",
        "        lemma.append(lemmatizer.lemmatize(word,pos=pos))\n",
        "    return lemma\n",
        "\n",
        "train_data['Questions']=train_data['Questions'].apply(lambda x : lemmatization(x.split()))\n",
        "train_data[\"Questions\"]=train_data[\"Questions\"].apply(lambda x : \" \".join(x))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pL0HGU975TWq"
      },
      "outputs": [],
      "source": [
        "#Analysing certain parameters about the train data\n",
        "\n",
        "def tokenize_question(text):\n",
        "    return text.split()\n",
        "\n",
        "questions = train_data['Questions'].tolist()\n",
        "\n",
        "print('The total number of words in the data is: ', sum([len(text.split()) for text in questions]))\n",
        "\n",
        "\n",
        "\n",
        "question_vect = CountVectorizer(tokenizer=tokenize_question)\n",
        "questions=question_vect.fit_transform(questions)\n",
        "\n",
        "print('The number of words in the vocabulary is: ', len(question_vect.vocabulary_))\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3vvD1AOZOvMk"
      },
      "outputs": [],
      "source": [
        "tags = train_data['Tags'].tolist()\n",
        "tags_Freq=Counter(tags)\n",
        "\n",
        "print(\"Total number of unique tags : \",len(tags_Freq.keys()))\n",
        "tags2=zip(tags_Freq.keys(),tags_Freq.values())\n",
        "\n",
        "tags2=sorted(tags2,key=lambda x:x[1],reverse=True)\n",
        "total_frequency=sum(tags_Freq.values())\n",
        "current=0\n",
        "idx=0\n",
        "while current/total_frequency<=0.95:\n",
        "    current+=tags2[idx][1]\n",
        "    if(idx>=19):\n",
        "      break\n",
        "    idx+=1\n",
        "\n",
        "print(f\"Number of tags that account for {round(current*100/total_frequency,2)}% of all tags appearance : \",idx+1)\n",
        "\n",
        "top_300_tags=[tags2[j][0] for j in range(20)]\n",
        "top_300_tags_values=[tags_Freq[tags2[j][0]] for j in range(20)]\n",
        "\n",
        "# plt.bar(top_300_tags, top_300_tags_values, width=0.5, color='r')\n",
        "# plt.xlabel('Tags')\n",
        "# plt.ylabel('Frequencies')\n",
        "# plt.title('Top 20 Tags and Frequencies')\n",
        "# plt.xticks(rotation=90)  # Rotate the x-axis labels for better visibility\n",
        "# plt.tight_layout()  # Adjust the layout to prevent label cutoff\n",
        "# plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Hgi5wKjB9gw"
      },
      "outputs": [],
      "source": [
        "\n",
        "train_new=train_data.copy()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KKLqkfsa-bVI"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def tokenize_question(text):\n",
        "    return text.split()\n",
        "\n",
        "def filter_number_features(name):\n",
        "  if name[0] in '0123456789' or len(name)<=3:\n",
        "    return False\n",
        "  return True\n",
        "tfidf_vect = TfidfVectorizer(tokenizer=tokenize_question,\n",
        "                               stop_words='english',\n",
        "                               min_df=4,\n",
        "                               max_df=0.5,max_features=1000)\n",
        "\n",
        "X_train_tfidf = tfidf_vect.fit_transform(train_new[\"Questions\"]).todense()\n",
        "# print('The number of words in the vocabulary is: ', len(tfidf_vect.vocabulary_))\n",
        "\n",
        "\n",
        "\n",
        "#get the feature names\n",
        "feature_names=tfidf_vect.get_feature_names_out()\n",
        "\n",
        "# Get the IDF scores\n",
        "idf_scores = tfidf_vect.idf_\n",
        "\n",
        "Final_Feature_Set=[]\n",
        "for idx,feature_name in enumerate(feature_names):\n",
        "    if filter_number_features(feature_name):\n",
        "       Final_Feature_Set.append([feature_name,idf_scores[idx]])\n",
        "\n",
        "Final_Feature_Set=sorted(Final_Feature_Set,key=lambda x :x[1],reverse=True)\n",
        "Final_Feature_Set=[x[0] for x in Final_Feature_Set]\n",
        "\n",
        "\n",
        "#Another approach (tfidf vectorization as feature set)\n",
        "\n",
        "df_train = pd.DataFrame(X_train_tfidf, columns=tfidf_vect.get_feature_names_out())\n",
        "\n",
        "df_train=df_train[Final_Feature_Set]\n",
        "\n",
        "df_train[\"Tags\"]=train_new[\"Tags\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "__h27EVHE_DC"
      },
      "outputs": [],
      "source": [
        "def tokenize_question(text):\n",
        "      return text.split()\n",
        "\n",
        "def CountVectorizer_Custom(data,Features):\n",
        "\n",
        "  questions = data['Questions'].tolist()\n",
        "  tags = data['Tags'].tolist()\n",
        "\n",
        "  question_vect = CountVectorizer(tokenizer=tokenize_question,binary=True,vocabulary=Features)\n",
        "  questions=question_vect.fit_transform(questions)\n",
        "\n",
        "  df_train = pd.DataFrame(questions.toarray(), columns=question_vect.get_feature_names_out())\n",
        "  df_train[\"Tags\"]=tags\n",
        "  return df_train\n",
        "\n",
        "#using count vectorizer as feature set\n",
        "\n",
        "train_new=CountVectorizer_Custom(train_new,Final_Feature_Set)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FJdoFeOnaMV9"
      },
      "outputs": [],
      "source": [
        "def filter_data_by_most_common_tags(data, common_tags):\n",
        "    filtered_data = data[data[\"Tags\"].isin(common_tags)]\n",
        "    return filtered_data\n",
        "\n",
        "\n",
        "def one_hot(column,data): #count vectorizer feature set\n",
        "  # Perform one-hot encoding using get_dummies()\n",
        "  one_hot_encoded = pd.get_dummies(data[column],prefix=\"tag\")\n",
        "\n",
        "  # Concatenate the one-hot encoded columns with the original dataframe\n",
        "  data_extended = pd.concat([data, one_hot_encoded], axis=1)\n",
        "  data_extended.drop(['Tags'],inplace=True,axis=1)\n",
        "\n",
        "  return data_extended\n",
        "\n",
        "def label_encoding(data, most_common_tags):#tfidf vectorizer\n",
        "    v = {}\n",
        "    for j in range(len(most_common_tags)):\n",
        "        v[most_common_tags[j]] = j\n",
        "    data[\"Tags\"] = data[\"Tags\"].apply(lambda x: v[x] if x in v else -1)\n",
        "    return data\n",
        "\n",
        "\n",
        "Final_train=filter_data_by_most_common_tags(df_train,top_300_tags)\n",
        "\n",
        "#performing the one hot encoding of the data\n",
        "\n",
        "\n",
        "Final_train=one_hot(\"Tags\",Final_train)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Another Approach (tfidf vectorizer)\n",
        "# Final_train=label_encoding(Final_train,most_common_tags)\n",
        "# Final_test=label_encoding(Final_test,most_common_tags)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9xnidnX9ZE4Y"
      },
      "outputs": [],
      "source": [
        "Final_train.to_csv(\"Final_train.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hD64LCASvqtr"
      },
      "outputs": [],
      "source": [
        "Final_train=pd.read_csv(\"Final_train.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XAUCsMoSyYYr"
      },
      "outputs": [],
      "source": [
        "# files.download('Final_train.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "apsDbB_WeOIU"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fEhjmrlXvQAh"
      },
      "outputs": [],
      "source": [
        "X_train=Final_train.iloc[:,:-1*len(top_300_tags)]\n",
        "\n",
        "Y_train=Final_train.iloc[:,-1*len(top_300_tags):]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HDT62ACPaSk-"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, Y_train, Y_test = train_test_split(X_train, Y_train, test_size=0.2, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DBgUUK3dyneL"
      },
      "outputs": [],
      "source": [
        "#function to calculate the evaluation metrics\n",
        "\n",
        "def eval_metrics(y_test, y_predicted, print_metrics=True):\n",
        "\n",
        "    accuracy = accuracy_score(y_test, y_predicted)\n",
        "    precision = precision_score(y_test, y_predicted, average='weighted')\n",
        "    recall = recall_score(y_test, y_predicted, average='weighted')\n",
        "    f1 = f1_score(y_test, y_predicted, average='weighted')\n",
        "\n",
        "    if print_metrics:\n",
        "        print(\"f1: %.3f - precision: %.3f - recall: %.3f - accuracy: %.3f\" % (\n",
        "            f1, precision, recall, accuracy))\n",
        "    return f1, precision, recall, accuracy\n",
        "def j_score(y_true, y_pred):\n",
        "  jaccard = np.minimum(y_true, y_pred).sum(axis = 1)/np.maximum(y_true, y_pred).sum(axis = 1)\n",
        "  return jaccard.mean()*100\n",
        "\n",
        "\n",
        "def print_score(y_pred,y_test):\n",
        "  print('Jacard score: {}'.format(j_score(y_test, y_pred)))\n",
        "  print('----')\n",
        "\n",
        "def convert(pred,original):\n",
        "    original=[np.argmax(original.iloc[idx,:]) for idx in range(original.shape[0])]\n",
        "    pred=[np.argmax(pred[idx,:]) for idx in range(pred.shape[0])]\n",
        "    return original,pred\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8aJNW7iiwCQa"
      },
      "source": [
        "**Machine learning using classification algos**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "npYTf3n9xBFB"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# Classifier - Algorithm - Logistic Regression\n",
        "\n",
        "log_clf = OneVsRestClassifier(LogisticRegression())\n",
        "\n",
        "log_clf.fit(X_train, Y_train)\n",
        "\n",
        "Y_train_predict = log_clf.predict(X_train)\n",
        "eval_metrics(Y_train, Y_train_predict)\n",
        "\n",
        "Y_test_predict=log_clf.predict(X_test)\n",
        "eval_metrics(Y_test,Y_test_predict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C8TB-KNa7Wzu"
      },
      "outputs": [],
      "source": [
        "# Classifier - Algorithm - SVM\n",
        "\n",
        "SVM = OneVsRestClassifier(svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto'))\n",
        "SVM.fit(X_train,Y_train)\n",
        "\n",
        "# predict the labels on validation dataset\n",
        "predictions_SVM_test = SVM.predict(X_test)\n",
        "predictions_SVM_train = SVM.predict(X_train)\n",
        "\n",
        "# Use accuracy_score function to get the accuracy\n",
        "print(\"SVM train Accuracy Score -> \",accuracy_score(predictions_SVM_train, Y_train)*100)\n",
        "print(\"SVM test Accuracy Score -> \",accuracy_score(predictions_SVM_test, Y_test)*100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RvS0elEtfNdt"
      },
      "outputs": [],
      "source": [
        "# NB classifier\n",
        "Naive = OneVsRestClassifier(naive_bayes.MultinomialNB())\n",
        "\n",
        "Naive.fit(X_train,Y_train)\n",
        "# predict the labels on validation dataset\n",
        "\n",
        "predictions_NB_test = Naive.predict(X_test)\n",
        "predictions_NB_train = Naive.predict(X_train)\n",
        "# Use accuracy_score function to get the accuracy\n",
        "\n",
        "print(\"Naive Bayes train Accuracy Score -> \",accuracy_score(predictions_NB_train, Y_train)*100)\n",
        "print(\"Naive Bayes test Accuracy Score -> \",accuracy_score(predictions_NB_test, Y_test)*100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5UqTgzc6yneP"
      },
      "source": [
        "**Neural Network part (RNN)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-OS-Icq-yneP"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Convert data to numpy arrays\n",
        "X_train = np.array(X_train)\n",
        "X_test = np.array(X_test)\n",
        "\n",
        "# Reshape the data for LSTM input\n",
        "X_train_lstm = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))\n",
        "X_test_lstm = np.reshape(X_test, (X_test.shape[0], 1, X_train.shape[1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K_xjTgctJNu1"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Define and compile the RNN model\n",
        "def build_model(units=64, dropout_rate=0.2,reg_lambda=0.001, activation='relu',optimizer='adam'):\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(units, input_shape=(X_train_lstm.shape[1], X_train_lstm.shape[2]), kernel_regularizer=l2(reg_lambda)))\n",
        "    model.add(Dropout(dropout_rate))\n",
        "    model.add(Dense(Y_train.shape[1], activation=activation))\n",
        "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "\n",
        "# Create KerasClassifier wrapper\n",
        "model = KerasClassifier(build_fn=build_model)\n",
        "\n",
        "# Define parameter grid\n",
        "param_grid = {\n",
        "    'epochs': [30,50],\n",
        "    'batch_size':[16,32,64],\n",
        "    'optimizer': ['adam', 'rmsprop'],\n",
        "    'reg_lambda':[0.001],\n",
        "    'activation': ['relu'],\n",
        "    'dropout_rate': [0.2],\n",
        "    'units': [64]\n",
        "}\n",
        "\n",
        "\n",
        "# Perform GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=3)\n",
        "grid_search.fit(X_train_lstm, Y_train)\n",
        "\n",
        "\n",
        "# Get the best parameters and best score\n",
        "best_params = grid_search.best_params_\n",
        "best_score = grid_search.best_score_\n",
        "\n",
        "print(\"Best Parameters: \", best_params)\n",
        "print(\"Best Score: \", best_score)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sx7FMU046r4z"
      },
      "outputs": [],
      "source": [
        "# best_params = grid_search.best_params_\n",
        "# best_score = grid_search.best_score_\n",
        "\n",
        "# print(\"Best Parameters: \", best_params)\n",
        "# print(\"Best Score: \", best_score)\n",
        "\n",
        "best_params={'activation': 'relu', 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'optimizer': 'rmsprop', 'reg_lambda': 0.001, 'units': 64}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zVuiNL6SyneS"
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "model.add(LSTM(best_params['units'], input_shape=(X_train_lstm.shape[1], X_train_lstm.shape[2]), kernel_regularizer=l2(best_params['reg_lambda'])))\n",
        "model.add(Dropout(best_params['dropout_rate']))\n",
        "model.add(Dense(Y_train.shape[1], activation=best_params['activation']))\n",
        "model.compile(loss='binary_crossentropy', optimizer=best_params['optimizer'], metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bunUn2UVyneS"
      },
      "outputs": [],
      "source": [
        "model.fit(X_train_lstm, Y_train, batch_size=best_params['batch_size'], epochs=best_params['epochs'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Nhg87QtyneT"
      },
      "outputs": [],
      "source": [
        "#training data results\n",
        "Y_pred_train=model.predict(X_train_lstm)\n",
        "original_train,pred_train=convert(Y_pred_train,Y_train)\n",
        "eval_metrics(pred_train,original_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fDT19Rouynek"
      },
      "outputs": [],
      "source": [
        "#test data results\n",
        "\n",
        "Y_pred_test=model.predict(X_test_lstm)\n",
        "original_test,pred_test=convert(Y_pred_test,Y_test)\n",
        "eval_metrics(pred_test,original_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "je4z3JKEynel"
      },
      "source": [
        "**Pytorch Neural Network**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZwxpT-2mynem"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "input_size = 969\n",
        "hidden_size = 250\n",
        "num_layers = 1\n",
        "output_size = 20\n",
        "n_epoch = 1000\n",
        "NN = nn.Sequential(\n",
        "    nn.RNN(input_size, output_size, num_layers, batch_first=False))\n",
        "\n",
        "# X_train = X_train.values.tolist()\n",
        "# Y_train = Y_train.values.tolist()\n",
        "X_train = torch.FloatTensor(X_train)\n",
        "Y_train = torch.FloatTensor(Y_train)\n",
        "\n",
        "optimizer = torch.optim.Adam(NN.parameters(), lr=0.1)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "for epoch in range(n_epoch):\n",
        "    # Forward pass\n",
        "    outputs = NN(X_train)\n",
        "    loss = criterion(outputs[0], Y_train)\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch+1) % 10 == 0:\n",
        "        print(f'Epoch [{epoch+1}/n_epoch], Loss: {loss.item():.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kOrpjQuqynen"
      },
      "outputs": [],
      "source": [
        "outputs[0].size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3gPM1qCoyneo"
      },
      "outputs": [],
      "source": [
        "type(outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UQ1jizN8ynep"
      },
      "outputs": [],
      "source": [
        "outputs"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a6laMgkCWwZd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CNN code (not optimized)**"
      ],
      "metadata": {
        "id": "n-huWltxWw6S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python\n",
        "# coding: utf-8\n",
        "\n",
        "# In[1]:\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "from keras.layers import *\n",
        "from keras.models import *\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from sklearn import preprocessing\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "labelencoder = LabelEncoder()\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "enc = OneHotEncoder()\n",
        "\n",
        "\n",
        "# In[2]:\n",
        "\n",
        "\n",
        "train_data = pd.read_csv(r'C:\\Users\\dishant\\Downloads\\train_data.csv')\n",
        "test_data = pd.read_csv(r'C:\\Users\\dishant\\Downloads\\test_data.csv')\n",
        "\n",
        "\n",
        "# In[3]:\n",
        "\n",
        "\n",
        "X = train_data['Questions']\n",
        "y = train_data['Tags']\n",
        "Y = pd.get_dummies(y)\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test , y_train, y_test = train_test_split(X, Y , test_size = 0.20)\n",
        "\n",
        "\n",
        "# In[4]:\n",
        "\n",
        "\n",
        "word_index = X_train.str.split()\n",
        "\n",
        "\n",
        "# In[5]:\n",
        "\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from keras_preprocessing.sequence import pad_sequences\n",
        "vocab_size = 5000\n",
        "oov_token = \"<OOV>\"\n",
        "tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_token)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "X_train_sequences = tokenizer.texts_to_sequences(X_train)\n",
        "X_test_sequences = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "max_length = 300\n",
        "padding_type = \"post\"\n",
        "trunction_type=\"post\"\n",
        "X_train_padded = pad_sequences(X_train_sequences,maxlen=max_length, padding=padding_type,\n",
        "                       truncating=trunction_type)\n",
        "X_test_padded = pad_sequences(X_test_sequences,maxlen=max_length,\n",
        "                               padding=padding_type, truncating=trunction_type)\n",
        "\n",
        "\n",
        "# In[7]:\n",
        "\n",
        "\n",
        "import os\n",
        "import zipfile\n",
        "with zipfile.ZipFile(r'C:\\Users\\dishant\\Downloads\\glove.42B.300d.zip') as zip_ref:\n",
        "    zip_ref.extractall(r'C:\\Users\\dishant\\Downloads\\glove')\n",
        "\n",
        "\n",
        "# In[8]:\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "embeddings_index = {}\n",
        "f = open(r'C:\\Users\\dishant\\Downloads\\glove\\glove.42B.300d.txt',encoding='utf-8')\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('Found %s word vectors.' % len(embeddings_index))\n",
        "\n",
        "\n",
        "# In[9]:\n",
        "\n",
        "\n",
        "count = 0;\n",
        "for i, word in word_index.items():\n",
        "    for j in word:\n",
        "        count = count+1\n",
        "\n",
        "\n",
        "# In[10]:\n",
        "\n",
        "\n",
        "embedding_matrix = np.zeros((count, 300))\n",
        "c = 0;\n",
        "for i, word in word_index.items():\n",
        "    for j in word:\n",
        "        embedding_vector = embeddings_index.get(j)\n",
        "        if embedding_vector is not None:\n",
        "        # words not found in embedding index will be all-zeros.\n",
        "             embedding_matrix[c] = embedding_vector\n",
        "             c = c+1\n",
        "        else:\n",
        "            c = c+1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# In[11]:\n",
        "\n",
        "\n",
        "embedding_layer = Embedding(input_dim=count,\n",
        "                            output_dim=max_length,\n",
        "                            weights=[embedding_matrix],\n",
        "                            input_length=max_length,\n",
        "                            trainable=False)\n",
        "\n",
        "\n",
        "# In[33]:\n",
        "\n",
        "\n",
        "model = Sequential([\n",
        "    embedding_layer,\n",
        "  Conv1D(2000, 5, activation='relu'),\n",
        "    GlobalMaxPooling1D(),\n",
        "  Dense(2000, activation='relu'),\n",
        "  Dense(1927, activation='sigmoid')\n",
        "])\n",
        "\n",
        "\n",
        "# In[34]:\n",
        "\n",
        "\n",
        "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "\n",
        "\n",
        "# In[35]:\n",
        "\n",
        "\n",
        "X_train_padded = preprocessing.normalize(X_train_padded)\n",
        "X_test_padded = preprocessing.normalize(X_test_padded)\n",
        "\n",
        "\n",
        "# In[36]:\n",
        "\n",
        "\n",
        "y_train.shape\n",
        "\n",
        "\n",
        "# In[37]:\n",
        "\n",
        "\n",
        "history = model.fit(X_train_padded, y_train, epochs=20, validation_data=(X_test_padded, y_test))\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "loss, accuracy = model.evaluate(X_test_padded,y_test)\n",
        "print('Testing Accuracy is {} '.format(accuracy*100))\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n"
      ],
      "metadata": {
        "id": "_tp6ykETWTb0"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}